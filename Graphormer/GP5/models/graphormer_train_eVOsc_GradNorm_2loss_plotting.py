import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from Graphormer.GP5.data_prepare.Dataloader_2 import SMILESDataset, collate_fn
from Graphormer.GP5.models.graphormer import GraphormerModel
import os
from Graphormer.GP5.Custom_Loss.custom_loss import fastdtw_loss
from Graphormer.GP5.Custom_Loss.SID_loss import SIDLoss
import json
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from Graphormer.GP5.Custom_Loss.fast_dtw import fastdtw
import time
from chemprop.train.loss_functions import sid_loss
from torch.cuda.amp import autocast, GradScaler
from tslearn.metrics import SoftDTWLossPyTorch
from Graphormer.GP5.Custom_Loss.GradNorm import GradNorm
import math
import matplotlib.pyplot as plt
from Graphormer.GP5.Custom_Loss.soft_dtw_cuda import SoftDTW


os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
SAVE_DIR = "./predictions"
os.makedirs(SAVE_DIR, exist_ok=True)

# âœ… í•™ìŠµ ì¢…ë£Œ í›„ ê°œë³„ í”Œë¡¯ ì €ì¥
def save_predictions(all_true_ex, all_pred_ex, all_true_prob, all_pred_prob, epoch):
    print("Save Process")
    num_samples = len(all_true_ex)

    for i in range(num_samples):
        print(f"save {i}")
        fig, axes = plt.subplots(3, 1, figsize=(8, 12))  # âœ… ì„¸ë¡œ í¬ê¸° í™•ëŒ€

        # âœ… Ex ê°’ í”Œë¡¯
        axes[0].plot(all_true_ex[i], label="True Ex", marker='o', linestyle='-', color='blue', alpha=0.7)
        axes[0].plot(all_pred_ex[i], label="Predicted Ex", marker='x', linestyle='-', color='orange', alpha=0.7)
        axes[0].set_title(f"Sample {i + 1} (Ex) - Epoch {epoch}")  # âœ… Epoch ì¶”ê°€
        axes[0].set_xlabel("Sample Index")
        axes[0].set_ylabel("Ex Value")
        axes[0].legend()
        axes[0].grid()

        # âœ… Prob ê°’ í”Œë¡¯
        axes[1].plot(all_true_prob[i], label="True Prob", marker='o', linestyle='-', color='green', alpha=0.7)
        axes[1].plot(all_pred_prob[i], label="Predicted Prob", marker='x', linestyle='-', color='red', alpha=0.7)
        axes[1].set_title(f"Sample {i + 1} (Prob) - Epoch {epoch}")  # âœ… Epoch ì¶”ê°€
        axes[1].set_xlabel("Sample Index")
        axes[1].set_ylabel("Prob Value")
        axes[1].legend()
        axes[1].grid()

        # âœ… ìˆ˜ì •ëœ ì¶”ê°€ í”Œë¡¯ (xì¶•: eV, yì¶•: Prob)
        axes[2].scatter(all_true_ex[i], all_true_prob[i], color='blue', label="Actual", marker='x', alpha=0.7)
        axes[2].scatter(all_pred_ex[i], all_pred_prob[i], color='red', label="Predicted", marker='o', alpha=0.7)

        for x, y in zip(all_true_ex[i], all_true_prob[i]):
            axes[2].vlines(x, ymin=0, ymax=y, color='blue', linestyle='-', alpha=0.5)

        for x, y in zip(all_pred_ex[i], all_pred_prob[i]):
            axes[2].vlines(x, ymin=0, ymax=y, color='red', linestyle='--', alpha=0.5)

        axes[2].set_title(f"Sample {i + 1} (eV vs Prob) - Epoch {epoch}")  # âœ… Epoch ì¶”ê°€
        axes[2].set_xlabel("Ex Value (eV)")
        axes[2].set_ylabel("Prob Value")
        axes[2].legend()
        axes[2].grid()

        plt.tight_layout()

        # âœ… íŒŒì¼ëª…ì— epoch ì¶”ê°€
        filename = os.path.join(SAVE_DIR, f"prediction_epoch{epoch}_{i + 1}.png")
        plt.savefig(filename, dpi=150)
        plt.close(fig)

        print(f"Saved: {filename}")




def train_model_ex_porb(
    config,
    target_type="ex_prob",
    loss_function="MSE",
    loss_function_ex="SoftDTW",
    loss_function_prob="SoftDTW",
    weight_ex=0.5,
    num_epochs=10,
    batch_size=50,
    n_pairs = 1,
    learning_rate=0.001,
    dataset_path="../../data/data_example.csv",
    patience = 20,
    DATASET = None,
    plt_mode = None
):
    """
    Train the Graphormer model with specified configurations and return the final loss and evaluation metrics.

    Args:
        config (dict): Configuration for the Graphormer model.
        target_type (str): Target type ("default", "ex_prob", "nm_distribution").
        loss_function_ex (str): Loss function for 'ex'.
        loss_function_prob (str): Loss function for 'prob'.
        weight_ex (float): Weight for 'ex' loss. Weight for 'prob' will be 1 - weight_ex.
        num_epochs (int): Number of training epochs.
        batch_size (int): Batch size for DataLoader.
        n_pairs (int): Number of pairs for 'ex_prob' targets.
        learning_rate (float): Learning rate for the optimizer.
        dataset_path (str): Path to the dataset CSV file.

    Returns:
        dict: A dictionary containing the final loss and evaluation metrics.
    """
    # Initialize dataset and DataLoader
    if DATASET is None:
        dataset = SMILESDataset(csv_file=dataset_path, attn_bias_w=1.0, target_type=target_type)
    else:
        dataset = DATASET

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        collate_fn=lambda batch: collate_fn(batch, dataset, n_pairs=n_pairs),
    )
    #for data in dataloader:
    #    for key in data:
    #        print(key, data[key].shape)
    # Initialize the model, loss function, and optimizer
    model = GraphormerModel(config)
    #SoftDTWLoss = SoftDTW(use_cuda=True, gamma=1.0, bandwidth=None)
    SoftDTWLoss = SoftDTW(use_cuda=True, gamma=0.2, bandwidth=None, normalize=True)

    def loss_fn_gen(loss_fn):
        if loss_fn == 'MSE':
            return nn.MSELoss()
        elif loss_fn == 'MAE':
            return nn.L1Loss()
        elif loss_fn == 'SoftDTW':
            return SoftDTWLoss
        elif loss_fn == 'Huber':
            return nn.SmoothL1Loss()
        elif loss_fn == 'SID':
            def sid_loss_wrapper(model_spectra, target_spectra, mask, threshold):
                return sid_loss(model_spectra, target_spectra, mask, threshold)
            return sid_loss_wrapper

    criterion_ex = loss_fn_gen(loss_function_ex)
    criterion_prob = loss_fn_gen(loss_function_prob)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    loss_modifier = GradNorm(num_losses=2, alpha=0.12)


    # Device setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    scaler = GradScaler()

    weight_prob = 1 - weight_ex

    # ì†ì‹¤ ê°’ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    best_loss_ex = float('inf')
    best_loss_prob = float('inf')
    best_model_path = "./best_model.pth"
    best_epoch = 0
    patience = patience  # Early stopping patience ì„¤ì •

    # ê°œì„ ë˜ì§€ ì•Šì€ epoch ìˆ˜ ì¶”ì 
    ex_no_improve_count = 0
    prob_no_improve_count = 0

    # Training loop
    #loss_dict = {"ex_loss":[], "prob_loss":[], "total_loss":[]}
    loss_history = {"ex_loss": [], "prob_loss": [], "total_loss": [], "normalized_ex_loss":[], "normalized_prob_loss": []}
    weight_true = [0.5, 0.5]

    first_loss_ex = None
    first_loss_prob = None

    all_true_ex = []
    all_pred_ex = []
    all_true_prob = []
    all_pred_prob = []

    for epoch in range(num_epochs):
        now_time = time.time()
        model.train()
        epoch_loss = 0.0
        loss_ex_list = []
        loss_prob_list = []
        normalized_loss_ex_list = []
        normalized_loss_prob_list = []
        weight_list = []

        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            batched_data = {k: batch[k] for k in ["x", "adj", "in_degree", "out_degree", "spatial_pos", "attn_bias", "edge_input", "attn_edge_type"]}
            targets = batch["targets"]
            outputs = model(batched_data, targets=targets, target_type=target_type)

            if epoch == num_epochs - 1:
                print("SAVE")
                for i in range(outputs.size(0)):
                    y_true_ex = targets[i, :, 0].cpu().numpy()
                    y_pred_ex = outputs[i, :, 0].cpu().detach().numpy()
                    y_true_prob = targets[i, :, 1].cpu().numpy()
                    y_pred_prob = outputs[i, :, 1].cpu().detach().numpy()

                    all_true_ex.append(y_true_ex)
                    all_pred_ex.append(y_pred_ex)
                    all_true_prob.append(y_true_prob)
                    all_pred_prob.append(y_pred_prob)

            if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                print("NaN detected in model outputs!")
                print(f"Sample outputs: {outputs}")
                #print(targets)
                raise ValueError("NaN values found in model outputs, check data and model configuration.")

            # Compute loss
            if target_type == "ex_prob":
                # SID Lossë¥¼ ì‚¬ìš©í•  ê²½ìš° ë§ˆìŠ¤í¬ ìƒì„±
                if loss_function_ex == "SID":
                    #outputs_ex = torch.clamp(outputs[:, :, 0:1], min=1e-8)
                    outputs_ex = outputs[:, :, 0:1] + 1e-8
                    targets_ex = targets[:, :, 0:1] + 1e-8
                    threshold = 1e-8
                    mask_ex = torch.ones_like(outputs_ex, dtype=torch.bool)  # ëª¨ë“  ì˜ì—­ í¬í•¨ (ì¡°ê±´ë¶€ ìˆ˜ì • ê°€ëŠ¥)
                    loss_ex = torch.stack([
                        criterion_ex(outputs_ex[i].unsqueeze(0), targets_ex[i].unsqueeze(0), mask_ex[i],threshold)
                        for i in range(outputs_ex.size(0))
                    ]).mean()
                    if torch.isnan(loss_ex):
                        print(f"NaN detected in loss_ex at epoch {epoch + 1}")
                        print("outputs_ex:", outputs_ex)
                        print("targets_ex:", targets_ex)
                    if torch.isinf(loss_ex):
                        print(f"Inf detected in loss_ex at epoch {epoch + 1}")
                        print("outputs_ex:", outputs_ex)
                        print("targets_ex:", targets_ex)
                else:
                    outputs_ex = outputs[:, :, 0:1] + 1e-8
                    targets_ex = targets[:, :, 0:1] + 1e-8
                    loss_ex = torch.stack([
                        criterion_ex(outputs_ex[i].unsqueeze(0), targets_ex[i].unsqueeze(0))
                        for i in range(outputs_ex.size(0))
                    ]).mean()

                #outputs_prob = torch.sigmoid(outputs[:, :, 1:2])
                #targets_prob = torch.sigmoid(targets[:, :, 1:2])

                #outputs_prob = torch.clamp(outputs[:, :, 1:2], min=1e-8)
                #targets_prob = torch.clamp(targets[:, :, 1:2], min=1e-8)



                # SID Lossë¥¼ ì‚¬ìš©í•  ê²½ìš° ë§ˆìŠ¤í¬ ìƒì„±
                if loss_function_prob == "SID":
                    #outputs_prob = torch.clamp(outputs[:, :, 1:2], min=1e-8)
                    outputs_prob = outputs[:, :, 1:2] + 1e-8
                    targets_prob = targets[:, :, 1:2] + 1e-8
                    threshold = 1e-8
                    mask_prob = torch.ones_like(outputs_prob, dtype=torch.bool)  # ëª¨ë“  ì˜ì—­ í¬í•¨ (ì¡°ê±´ë¶€ ìˆ˜ì • ê°€ëŠ¥)
                    loss_prob = torch.stack([
                        criterion_prob(outputs_prob[i].unsqueeze(0), targets_prob[i].unsqueeze(0), mask_prob[i],threshold)
                        for i in range(outputs_prob.size(0))
                    ]).mean()
                    if torch.isnan(loss_prob):
                        print(f"NaN detected in loss_prob at epoch {epoch + 1}")
                        print("outputs_prob:", outputs_prob)
                        print("targets_prob:", targets_prob)
                    if torch.isinf(loss_prob):
                        print(f"Inf detected in loss_prob at epoch {epoch + 1}")
                        print("outputs_prob:", outputs_prob)
                        print("targets_prob:", targets_prob)
                else:
                    outputs_prob = outputs[:, :, 1:2] + 1e-8
                    targets_prob = targets[:, :, 1:2] + 1e-8
                    loss_prob = torch.stack([
                        criterion_prob(outputs_prob[i].unsqueeze(0), targets_prob[i].unsqueeze(0))
                        for i in range(outputs_prob.size(0))
                    ]).mean()
                #print(loss_ex.detach(),loss_prob.detach())

                # ì†ì‹¤ ê°’ì´ tensorì¸ ê²½ìš° ì •ê·œí™” ê°’ë„ tensorë¡œ ìœ ì§€
                if first_loss_ex is not None and isinstance(loss_ex, torch.Tensor):
                    normalized_loss_ex = loss_ex / first_loss_ex
                else:
                    normalized_loss_ex = loss_ex

                if first_loss_prob is not None and isinstance(loss_prob, torch.Tensor):
                    normalized_loss_prob = loss_prob / first_loss_prob
                else:
                    normalized_loss_prob = loss_prob

                weight = loss_modifier.compute_weights([loss_ex, loss_prob], model)
                #weight = loss_modifier.compute_weights([normalized_loss_ex, normalized_loss_prob], model)
                weight_list.append(weight)

                # Final loss ê³„ì‚°
                #loss = weight_true[0] * loss_ex + weight_true[1] * loss_prob
                loss = weight_true[0] * normalized_loss_ex + weight_true[1] * normalized_loss_prob
            else:
                raise ValueError("Invalid target type")

            if first_loss_ex is None:
                first_loss_ex = loss_ex.item()
            if first_loss_prob is None:
                first_loss_prob = loss_prob.item()

            normalized_loss_ex = loss_ex.item() / first_loss_ex if first_loss_ex != 0 else 1.0
            normalized_loss_prob = loss_prob.item() / first_loss_prob if first_loss_prob != 0 else 1.0

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            loss_ex_list.append(loss_ex.item())
            loss_prob_list.append(loss_prob.item())
            normalized_loss_ex_list.append(normalized_loss_ex)
            normalized_loss_prob_list.append(normalized_loss_prob)

        avg_epoch_loss = epoch_loss / len(dataloader)
        avg_loss_ex = np.mean(loss_ex_list)
        avg_loss_prob = np.mean(loss_prob_list)
        avg_normalized_loss_ex = np.mean(normalized_loss_ex_list)
        avg_normalized_loss_prob = np.mean(normalized_loss_prob_list)

        weight_true = torch.stack(weight_list).mean(dim=0).detach()

        loss_history["ex_loss"].append(avg_loss_ex)
        loss_history["prob_loss"].append(avg_loss_prob)
        loss_history["total_loss"].append(avg_epoch_loss)
        loss_history["normalized_ex_loss"].append(avg_normalized_loss_ex)
        loss_history["normalized_prob_loss"].append(avg_normalized_loss_prob)



        # Best model ì €ì¥ & Early stopping ì²´í¬
        # âœ… Early Stopping ê°œë³„ ì†ì‹¤ ê¸°ì¤€ ì ìš©
        #if avg_loss_ex < best_loss_ex:
        #    best_loss_ex = avg_loss_ex
        #    ex_no_improve_count = 0
        #    torch.save(model.state_dict(), "./best_model.pth")
        #else:
        #    ex_no_improve_count += 1
#
        #if avg_loss_prob < best_loss_prob:
        #    best_loss_prob = avg_loss_prob
        #    prob_no_improve_count = 0
        #    torch.save(model.state_dict(), "./best_model.pth")
        #else:
        #    prob_no_improve_count += 1

        # âœ… ë‘ ì†ì‹¤ ëª¨ë‘ patience ê¸°ì¤€ ì¶©ì¡± ì‹œ ì¢…ë£Œ
        #if ex_no_improve_count >= patience and prob_no_improve_count >= patience:
        #    print(f"Early stopping triggered at epoch {epoch + 1} (Both losses satisfied patience)")
        #    break

        epoch_time = time.time() - now_time
        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}, "
              f"Loss_Ex: {avg_loss_ex:.4f}, Loss_Prob: {avg_loss_prob:.4f}, "
              f"Normalized_Loss_Ex: {avg_normalized_loss_ex:.4f}, Normalized_Loss_Prob: {avg_normalized_loss_prob:.4f}, "
              f"Weights: {weight_true}, Time: {epoch_time:.2f}")



    save_predictions(all_true_ex, all_pred_ex, all_true_prob, all_pred_prob, epoch)

    # Final evaluation metrics ê³„ì‚°
    model.load_state_dict(torch.load(best_model_path,))
    model.eval()

    # ìŠ¤í™íŠ¸ëŸ¼ë³„ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    # Initialize lists for individual and combined metrics
    sid_spectrum_ex, sid_spectrum_prob, sid_spectrum_combined = [], [], []
    sis_spectrum_ex, sis_spectrum_prob, sis_spectrum_combined = [], [], []
    r2_spectrum_ex, r2_spectrum_prob, r2_spectrum_combined = [], [], []
    mae_spectrum_ex, mae_spectrum_prob, mae_spectrum_combined = [], [], []
    rmse_spectrum_ex, rmse_spectrum_prob, rmse_spectrum_combined = [], [], []
    softdtw_spectrum_ex, softdtw_spectrum_prob, softdtw_spectrum_combined = [], [], []
    fastdtw_spectrum_ex, fastdtw_spectrum_prob, fastdtw_spectrum_combined = [], [], []

    y_true_prob_nan_cases = []
    y_pred_prob_nan_cases = []
    all_cases_true = []
    all_cases_pred = []
    nan_cases_indices = []
    all_cases_true_ex = []
    all_cases_pred_ex = []

    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            batched_data = {k: batch[k] for k in
                            ["x", "adj", "in_degree", "out_degree", "spatial_pos", "attn_bias", "edge_input",
                             "attn_edge_type"]}
            targets = batch["targets"]
            outputs = model(batched_data, targets=targets, target_type=target_type)
            #outputs = torch.clamp(outputs, min=1e-8)
            if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                print("NaN detected in model outputs!")
                raise ValueError("NaN values found in model outputs, check data and model configuration.")

                # ğŸ”¥ ì˜ˆì¸¡ê°’ ì €ì¥ (ëª¨ë“  ë°°ì¹˜ì—ì„œ ì €ì¥)


            # Batchì—ì„œ ê° ìŠ¤í™íŠ¸ëŸ¼ë³„ë¡œ ê³„ì‚°
            for i in range(targets.size(0)):  # batch_size ë§Œí¼ ë£¨í”„
                y_true = targets[i].cpu().numpy()
                #print("y_true.shape)",y_true.shape)
                y_pred = outputs[i].cpu().detach().numpy()
                #print("y_pred.shape",y_pred.shape)
                if target_type == "ex_prob":
                    # 2D ì¸ë±ì‹±ì´ í•„ìš”í•œ ê²½ìš° (ë°°ì¹˜ í¬ê¸° x n_pairs x 2)
                    y_true_ex = y_true[:, 0]  # ex ê°’ë“¤
                    y_pred_ex = y_pred[:, 0]  # ì˜ˆì¸¡ëœ ex ê°’ë“¤
                    y_true_prob = y_true[:, 1]  # prob ê°’ë“¤
                    y_pred_prob = y_pred[:, 1]  # ì˜ˆì¸¡ëœ prob ê°’ë“¤

                    ## ìŒìˆ˜ ë°©ì§€
                    #y_pred_ex = np.clip(y_pred_ex, 1e-8, None)
                    #y_true_ex = np.clip(y_true_ex, 1e-8, None)
                    #y_pred_prob = np.clip(y_pred_prob, 1e-8, None)
                    #y_true_prob = np.clip(y_true_prob, 1e-8, None)
#
                    ## ì •ê·œí™” (í•©ì„ 1ë¡œ ë§ì¶”ê¸°)
                    #y_pred_ex /= np.sum(y_pred_ex)
                    #y_true_ex /= np.sum(y_true_ex)
                    #y_pred_prob /= np.sum(y_pred_prob)
                    #y_true_prob /= np.sum(y_true_prob)
                else:
                    raise ValueError(f"Unknown target_type: {target_type}")

                # SID ë° SIS ê³„ì‚°

                #print("sid shape",torch.tensor(y_pred_ex).unsqueeze(0).to(device).shape, torch.tensor(y_true_ex).unsqueeze(0).to(device).shape)
                #print(torch.ones_like(torch.tensor(y_pred_ex).unsqueeze(0), dtype=torch.bool).to(device).shape)
                sid_ex = sid_loss(torch.tensor(y_pred_ex).unsqueeze(0).to(device),
                                  torch.tensor(y_true_ex).unsqueeze(0).to(device),
                                  torch.ones_like(torch.tensor(y_pred_ex).unsqueeze(0), dtype=torch.bool).to(
                                      device),
                                  threshold=1e-8).mean().item()
                sid_prob = sid_loss(torch.tensor(y_pred_prob).unsqueeze(0).to(device),
                                    torch.tensor(y_true_prob).unsqueeze(0).to(device),
                                    torch.ones_like(torch.tensor(y_pred_prob).unsqueeze(0), dtype=torch.bool).to(
                                        device),
                                    threshold=1e-8).mean().item()

                sid_combined = sid_loss(torch.tensor(y_pred).unsqueeze(0).to(device),
                                        torch.tensor(y_true).unsqueeze(0).to(device),
                                        torch.ones_like(torch.tensor(y_pred).unsqueeze(0), dtype=torch.bool).to(
                                            device),
                                        threshold=1e-8).mean().item()

                # SIS ê³„ì‚°
                sis_ex = 1 / (1 + sid_ex)
                sis_prob = 1 / (1 + sid_prob)
                sis_combined = 1 / (1 + sid_combined)

                # ìŠ¤í™íŠ¸ëŸ¼ë³„ ê³„ì‚° ê²°ê³¼ ì €ì¥
                r2_ex = r2_score(y_true_ex, y_pred_ex)
                r2_prob = r2_score(y_true_prob, y_pred_prob)
                r2_combined = r2_score(y_true.flatten(), y_pred.flatten())

                mae_ex = mean_absolute_error(y_true_ex, y_pred_ex)
                mae_prob = mean_absolute_error(y_true_prob, y_pred_prob)
                mae_combined = mean_absolute_error(y_true.flatten(), y_pred.flatten())

                rmse_ex = mean_squared_error(y_true_ex, y_pred_ex, squared=False)
                rmse_prob = mean_squared_error(y_true_prob, y_pred_prob, squared=False)
                rmse_combined = mean_squared_error(y_true.flatten(), y_pred.flatten(), squared=False)

                #print("softdtw_ex = SoftDTWLoss",torch.tensor(y_pred).unsqueeze(0).unsqueeze(-1).shape)
                softdtw_ex = SoftDTWLoss(
                    torch.tensor(y_pred_ex).unsqueeze(0).unsqueeze(-1).to(device),
                    # (batch_size=1, seq_len, dimension=1)
                    torch.tensor(y_true_ex).unsqueeze(0).unsqueeze(-1).to(device),
                    # (batch_size=1, seq_len, dimension=1)
                ).item()
                softdtw_prob = SoftDTWLoss(
                    torch.tensor(y_pred_prob).unsqueeze(0).unsqueeze(-1).to(device),
                    torch.tensor(y_true_prob).unsqueeze(0).unsqueeze(-1).to(device)
                ).item()
                softdtw_combined = SoftDTWLoss(
                    torch.tensor(y_pred).unsqueeze(0).to(device),
                    torch.tensor(y_true).unsqueeze(0).to(device)
                ).item()

                fastdtw_ex, _ = fastdtw(torch.tensor(y_pred_ex), torch.tensor(y_true_ex))
                fastdtw_prob, _ = fastdtw(torch.tensor(y_pred_prob), torch.tensor(y_true_prob))
                fastdtw_combined, _ = fastdtw(torch.tensor(y_pred.flatten()), torch.tensor(y_true.flatten()))

                # Append results to respective lists
                if math.isnan(sid_prob):
                    print("sid_prob is nan!", sid_prob)
                    y_true_prob_nan_cases.append(y_true_prob)
                    y_pred_prob_nan_cases.append(y_pred_prob)

                sid_spectrum_combined.append(sid_combined)
                #print("sid_combined", sid_combined, type(sid_combined))
                if math.isnan(sid_combined):
                    print("sid_combined is nan", sid_combined, type(sid_combined))

                # ì „ì²´ ì¼€ì´ìŠ¤ prob ì €ì¥
                all_cases_true.append(y_true_prob)
                all_cases_pred.append(y_pred_prob)

                # ëª¨ë“  ex ê°’ ì €ì¥
                all_cases_true_ex.append(y_true_ex)
                all_cases_pred_ex.append(y_pred_ex)

                # NaN ë°œìƒ ì‹œ ê¸°ë¡
                if math.isnan(sid_prob):
                    nan_cases_indices.append(len(all_cases_true) - 1)  # í˜„ì¬ ì¼€ì´ìŠ¤ ì¸ë±ìŠ¤

                sis_spectrum_ex.append(sis_ex)
                sis_spectrum_prob.append(sis_prob)
                sis_spectrum_combined.append(sis_combined)

                r2_spectrum_ex.append(r2_ex)
                r2_spectrum_prob.append(r2_prob)
                r2_spectrum_combined.append(r2_combined)

                mae_spectrum_ex.append(mae_ex)
                mae_spectrum_prob.append(mae_prob)
                mae_spectrum_combined.append(mae_combined)

                rmse_spectrum_ex.append(rmse_ex)
                rmse_spectrum_prob.append(rmse_prob)
                rmse_spectrum_combined.append(rmse_combined)

                softdtw_spectrum_ex.append(softdtw_ex)
                softdtw_spectrum_prob.append(softdtw_prob)
                softdtw_spectrum_combined.append(softdtw_combined)

                fastdtw_spectrum_ex.append(fastdtw_ex)
                fastdtw_spectrum_prob.append(fastdtw_prob)
                fastdtw_spectrum_combined.append(fastdtw_combined)

    # ìŠ¤í™íŠ¸ëŸ¼ë³„ í‰ê·  ê³„ì‚°
    r2_avg_ex = np.mean(r2_spectrum_ex)
    r2_avg_prob = np.mean(r2_spectrum_prob)
    r2_avg_combined = np.mean(r2_spectrum_combined)

    mae_avg_ex = np.mean(mae_spectrum_ex)
    mae_avg_prob = np.mean(mae_spectrum_prob)
    mae_avg_combined = np.mean(mae_spectrum_combined)

    rmse_avg_ex = np.mean(rmse_spectrum_ex)
    rmse_avg_prob = np.mean(rmse_spectrum_prob)
    rmse_avg_combined = np.mean(rmse_spectrum_combined)

    softdtw_avg_ex = np.mean(softdtw_spectrum_ex)
    softdtw_avg_prob = np.mean(softdtw_spectrum_prob)
    softdtw_avg_combined = np.mean(softdtw_spectrum_combined)

    fastdtw_avg_ex = np.mean([x.cpu().numpy() if isinstance(x, torch.Tensor) else x for x in fastdtw_spectrum_ex])
    fastdtw_avg_prob = np.mean([x.cpu().numpy() if isinstance(x, torch.Tensor) else x for x in fastdtw_spectrum_prob])
    fastdtw_avg_combined = np.mean(
        [x.cpu().numpy() if isinstance(x, torch.Tensor) else x for x in fastdtw_spectrum_combined])

    print("sid_spectrum_ex", sid_spectrum_ex)
    sid_avg_ex = np.mean(sid_spectrum_ex)
    print("sid_spectrum_prob", sid_spectrum_prob)
    sid_avg_prob = np.mean(sid_spectrum_prob)
    print("sid_spectrum_combined", sid_spectrum_combined)
    sid_avg_combined = np.mean(sid_spectrum_combined)

    sis_avg_ex = np.mean(sis_spectrum_ex)
    sis_avg_prob = np.mean(sis_spectrum_prob)
    sis_avg_combined = np.mean(sis_spectrum_combined)
    results = {}
    # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ ìƒì„±
    results.update({
        "best_epoch": best_epoch,
        "r2_avg_ex": r2_avg_ex,
        "r2_avg_prob": r2_avg_prob,
        "r2_avg_combined": r2_avg_combined,
        "mae_avg_ex": mae_avg_ex,
        "mae_avg_prob": mae_avg_prob,
        "mae_avg_combined": mae_avg_combined,
        "rmse_avg_ex": rmse_avg_ex,
        "rmse_avg_prob": rmse_avg_prob,
        "rmse_avg_combined": rmse_avg_combined,
        "softdtw_avg_ex": softdtw_avg_ex,
        "softdtw_avg_prob": softdtw_avg_prob,
        "softdtw_avg_combined": softdtw_avg_combined,
        "fastdtw_avg_ex": fastdtw_avg_ex,
        "fastdtw_avg_prob": fastdtw_avg_prob,
        "fastdtw_avg_combined": fastdtw_avg_combined,
        "sid_avg_ex": sid_avg_ex,
        "sid_avg_prob": sid_avg_prob,
        "sid_avg_combined": sid_avg_combined,
        "sis_avg_ex": sis_avg_ex,
        "sis_avg_prob": sis_avg_prob,
        "sis_avg_combined": sis_avg_combined,
    })
    print("loss_history_training", loss_history)
    for key, value in loss_history.items():
        results[f"{key}_history_training"] = value

    # ê²°ê³¼ ì €ì¥
    #with open("./training_results.json", "w") as f:
    #    json.dump(results, f)
    print("Training complete.")
    return results, best_model_path

###############################################################################
config = {
    "num_atoms": 100,              # ë¶„ìì˜ ìµœëŒ€ ì›ì ìˆ˜ (ê·¸ë˜í”„ì˜ ë…¸ë“œ ê°œìˆ˜) 100
    "num_in_degree": 10,           # ê·¸ë˜í”„ ë…¸ë“œì˜ ìµœëŒ€ in-degree
    "num_out_degree": 10,          # ê·¸ë˜í”„ ë…¸ë“œì˜ ìµœëŒ€ out-degree
    "num_edges": 100,               # ê·¸ë˜í”„ì˜ ìµœëŒ€ ì—£ì§€ ê°œìˆ˜ 50
    "num_spatial": 100,            # ê³µê°„ì  ìœ„ì¹˜ ì¸ì½”ë”©ì„ ìœ„í•œ ìµœëŒ€ ê°’ default 100
    "num_edge_dis": 10,            # ì—£ì§€ ê±°ë¦¬ ì¸ì½”ë”©ì„ ìœ„í•œ ìµœëŒ€ ê°’
    "edge_type": "multi_hop",      # ì—£ì§€ íƒ€ì… ("multi_hop" ë˜ëŠ” ë‹¤ë¥¸ ê°’ ê°€ëŠ¥)
    "multi_hop_max_dist": 2,       # Multi-hop ì—£ì§€ì˜ ìµœëŒ€ ê±°ë¦¬
    "num_encoder_layers": 6,       # Graphormer ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ì¸ì½”ë” ë ˆì´ì–´ ê°œìˆ˜
    "embedding_dim": 128,          # ì„ë² ë”© ì°¨ì› í¬ê¸° (ë…¸ë“œ, ì—£ì§€ ë“±)
    "ffn_embedding_dim": 256,      # Feedforward Networkì˜ ì„ë² ë”© í¬ê¸°
    "num_attention_heads": 8,      # Multi-head Attentionì—ì„œ í—¤ë“œ ê°œìˆ˜
    "dropout": 0.1,                # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    "attention_dropout": 0.1,      # Attention ë ˆì´ì–´ì˜ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    "activation_dropout": 0.1,     # í™œì„±í™” í•¨ìˆ˜ ì´í›„ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    "activation_fn": "gelu",       # í™œì„±í™” í•¨ìˆ˜ ("gelu", "relu" ë“±)
    "pre_layernorm": False,        # LayerNormì„ Pre-Normalizationìœ¼ë¡œ ì‚¬ìš©í• ì§€ ì—¬ë¶€
    "q_noise": 0.0,                # Quantization noise (í›ˆë ¨ ì¤‘ ë…¸ì´ì¦ˆ ì¶”ê°€ë¥¼ ìœ„í•œ ë§¤ê°œë³€ìˆ˜)
    "qn_block_size": 8,            # Quantization block í¬ê¸°
    "output_size": 100,            # ëª¨ë¸ ì¶œë ¥ í¬ê¸°
}


# Example usage
if __name__ == "__main__":
    target_type = "ex_prob"  # "ex_prob" "default" "nm_distribution"
    loss_functions = ["SoftDTW"] # "MSE", "MAE", "SoftDTW", "Huber", "SID"

    for loss_ex in loss_functions:
        for loss_prob in loss_functions:
            print(f"Running training with loss_function_ex={loss_ex}, loss_function_prob={loss_prob}")
            final_loss = train_model_ex_porb(
                config=config,
                target_type=target_type,
                dataset_path="../../data/train_1000.csv",
                loss_function_ex=loss_ex,
                loss_function_prob=loss_prob,
                learning_rate=0.001,
                batch_size=50,
                num_epochs=250,
                n_pairs=10,
                patience=20,
                plt_mode = "all" # "epoch, all"
            )
            print(f"Final loss for loss_function_ex={loss_ex}, loss_function_prob={loss_prob}: {final_loss}")