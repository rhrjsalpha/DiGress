import time

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from Graphormer.GP5.models.graphormer_train_eVOsc_GradNorm_2loss_GPUs import train_model_ex_porb, evaluate_model
from sklearn.model_selection import KFold
from torch.utils.data import Subset
from torch.utils.data import DataLoader
from Graphormer.GP5.data_prepare.Dataloader_2 import SMILESDataset, collate_fn
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from types import SimpleNamespace
import os
import json
import subprocess

import pandas as pd
def run_ddp_training_external(args_dict, fold_idx):

    json_path = f"./ddp_args_fold{fold_idx}.json"
    # JSONì— ì €ì¥ ê°€ëŠ¥í•œ ê°’ë§Œ í•„í„°ë§
    json_safe_args = {
        k: v for k, v in args_dict.items()
        if k not in ["TRAIN_DATASET", "VAL_DATASET"]
    }

    with open(json_path, "w") as f:
        json.dump(json_safe_args, f)

    # subprocess ì‹¤í–‰
    command = f"python train_model_entrypoint.py --args {json_path}"
    print(f"ğŸ§ª Launching subprocess for Fold {fold_idx+1}: {command}")
    subprocess.run(command, shell=True, check=True)

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29501'
    # libuv ì‚¬ìš©ì„ ëª…ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”
    os.environ['USE_LIBUV'] = '0'

    dist.init_process_group(
        backend="gloo",
        rank=rank,
        world_size=world_size,
        init_method='tcp://127.0.0.1:29501'
    )
    torch.cuda.set_device(rank)

def flatten_cv_and_train_test(cv_df: pd.DataFrame, train_test_row: pd.Series) -> pd.DataFrame:
    result = {}

    for col in cv_df.columns:
        values = cv_df[col].tolist()

        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ì»¬ëŸ¼ì´ë©´ foldë³„ë¡œë§Œ ì €ì¥ (mean ë¶ˆê°€)
        if isinstance(values[0], str) and values[0].startswith("[") and values[0].endswith("]"):
            for i in range(5):
                result[f"{col}_CV{i+1}"] = values[i]
            # í‰ê· ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
        else:
            # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: foldë³„ + í‰ê· 
            for i in range(5):
                result[f"{col}_CV{i+1}"] = values[i]
            result[f"{col}_CV"] = pd.to_numeric(cv_df[col], errors="coerce").mean()

    # ì „ì²´ í•™ìŠµ ê²°ê³¼ ì €ì¥ (Train / Test êµ¬ë¶„)
    for col in train_test_row.index:
        if col.startswith("val_"):
            new_col = col.replace("val_", "") + "_Test"
        else:
            new_col = col + "_Train"
        result[new_col] = train_test_row[col]

    return pd.DataFrame([result])


def cross_validate_with_splits(train_val_splits, config, target_type, loss_function_ex, loss_function_prob,
                               num_epochs, batch_size, n_pairs, learning_rate, patience, alpha,
                               training_result_root="./cv_result.csv", train_dataset_path=None, val_dataset_path=None, use_ddp=True):
    all_paths = []

    for fold, (train_dataset, val_dataset) in enumerate(train_val_splits):
        print(f"ğŸš€ Fold {fold+1} ì‹œì‘")
        start_time = time.time()
        world_size = torch.cuda.device_count()

        fold_result_path = f"./fold{fold+1}.csv"

        train_args = SimpleNamespace(
            world_size=world_size,
            config=config,
            train_dataset_path=train_dataset_path,
            val_dataset_path=val_dataset_path,
            target_type=target_type,
            loss_function_ex=loss_function_ex,
            loss_function_prob=loss_function_prob,
            num_epochs=num_epochs,
            batch_size=batch_size,
            n_pairs=n_pairs,
            learning_rate=learning_rate,
            patience=patience,
            alpha=alpha,
            TRAIN_DATASET=train_dataset,
            VAL_DATASET=val_dataset,
            training_result_root=f"./fold{fold+1}.csv",
            skip_setup=False
        )

        if use_ddp == True:
            #run_ddp_training_external(train_args.__dict__, fold_idx=fold)
            mp.spawn(train_model_ex_porb, args=(train_args,), nprocs=train_args.world_size)
        else:
            train_model_ex_porb(0, train_args)

        # âœ… í›ˆë ¨ ê²°ê³¼ íŒŒì¼ì—ì„œ best_model_path, best_epoch, loss_history, weight_history ìˆ˜ë™ìœ¼ë¡œ ì¶”ì 
        best_model_path = "./best_model.pth"  # ê¸°ë³¸ ì €ì¥ ê²½ë¡œì—ì„œ ë¡œë”©
        best_epoch = train_args.num_epochs  # í˜¹ì€ ê¸°ë¡í•œ best_epoch ë¶ˆëŸ¬ì˜¤ê¸°
        # ì•„ë˜ëŠ” ì‹¤ì œë¡œ train_model_ex_porbê°€ ë°˜í™˜í•œ ê±¸ ë°›ìœ¼ë©´ ì¢‹ì§€ë§Œ, spawnì€ ê°’ì„ ì§ì ‘ return ëª» í•¨

        # âœ… í‰ê°€ë¥¼ ìœ„í•œ ë¡œë” ìƒì„± (ë‹¨ì¼ GPUì—ì„œ ìˆ˜í–‰)
        train_loader = DataLoader(train_dataset, batch_size=batch_size,
                                  collate_fn=lambda batch: collate_fn(batch, train_dataset.dataset, n_pairs=n_pairs))

        val_loader = DataLoader(val_dataset, batch_size=batch_size,
                                collate_fn=lambda batch: collate_fn(batch, val_dataset.dataset, n_pairs=n_pairs))

        loss_df = pd.read_csv(f"./loss_history_rank0.csv")
        weight_df = pd.read_csv(f"./weight_history_rank0.csv")
        best_info = pd.read_csv(f"./best_info_rank0.csv").iloc[0]

        loss_history = loss_df.to_dict(orient="list")
        weight_history = weight_df.to_dict(orient="list")
        best_model_path = best_info["best_model_path"]
        best_epoch = int(best_info["best_epoch"])

        # âœ… ë‹¨ì¼ GPU í‰ê°€ ìˆ˜í–‰
        train_args.skip_setup = True  # DDP ì´ˆê¸°í™” ì•ˆí•¨
        evaluate_model(best_model_path, train_loader, val_loader, train_args, best_epoch, loss_history,
                       weight_history)

        all_paths.append(fold_result_path)
        total_time = time.time() - start_time
        print(f"ğŸš€ Fold {fold + 1} end, time:{total_time}")

    # Foldë³„ ê²°ê³¼ í†µí•©
    merged = [pd.read_csv(p) for p in all_paths]
    result = pd.concat(merged, axis=0, ignore_index=True)

    # í‰ê·  ê²°ê³¼ ì¶”ê°€
    mean_row = result.mean(numeric_only=True)
    result = pd.concat([result, pd.DataFrame([mean_row])], ignore_index=True)
    result.to_csv(training_result_root, index=False)
    print(f"âœ… CV ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {training_result_root}")

def train_on_full_and_eval(config, full_train_dataset, test_dataset, target_type, loss_function_ex, loss_function_prob,
                           num_epochs, batch_size, n_pairs, learning_rate, patience, alpha,
                           training_result_root="./final_train_results.csv", use_ddp=None):
    print("ğŸ§ª ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ í›„ í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€")
    start_time = time.time()
    world_size = torch.cuda.device_count()
    train_args = SimpleNamespace(
        world_size=world_size,
        config=config,
        train_dataset_path=None,
        val_dataset_path=None,
        target_type=target_type,
        loss_function_ex=loss_function_ex,
        loss_function_prob=loss_function_prob,
        num_epochs=num_epochs,
        batch_size=batch_size,
        n_pairs=n_pairs,
        learning_rate=learning_rate,
        patience=patience,
        alpha=alpha,
        TRAIN_DATASET=full_train_dataset,
        VAL_DATASET=test_dataset,
        training_result_root=training_result_root,
        skip_setup=False,
    )

    if use_ddp == True:
        mp.spawn(train_model_ex_porb, args=(train_args,), nprocs=train_args.world_size)
    else:
        train_model_ex_porb(0, train_args)

    # âœ… í‰ê°€ë¥¼ ìœ„í•œ ë¡œë” ìƒì„± (ë‹¨ì¼ GPUì—ì„œ ìˆ˜í–‰)
    train_loader = DataLoader(full_train_dataset, batch_size=batch_size,
                              collate_fn=lambda batch: collate_fn(batch, full_train_dataset, n_pairs=n_pairs))

    val_loader = DataLoader(test_dataset, batch_size=batch_size,
                            collate_fn=lambda batch: collate_fn(batch, test_dataset, n_pairs=n_pairs))

    loss_df = pd.read_csv(f"./loss_history_rank0.csv")
    weight_df = pd.read_csv(f"./weight_history_rank0.csv")
    best_info = pd.read_csv(f"./best_info_rank0.csv").iloc[0]

    loss_history = loss_df.to_dict(orient="list")
    weight_history = weight_df.to_dict(orient="list")
    best_model_path = best_info["best_model_path"]
    best_epoch = int(best_info["best_epoch"])

    # âœ… ë‹¨ì¼ GPU í‰ê°€ ìˆ˜í–‰
    train_args.skip_setup = True  # DDP ì´ˆê¸°í™” ì•ˆí•¨
    evaluate_model(best_model_path, train_loader, val_loader, train_args, best_epoch, loss_history,
                   weight_history)

    total_time = time.time() - start_time
    print(f"full training end, time:{total_time}")

def merge_all_results(cv_csv_path, test_csv_path, final_output_path="final_pipeline_results.csv"):
    df_cv = pd.read_csv(cv_csv_path)
    df_test = pd.read_csv(test_csv_path)
    df_all = pd.concat([df_cv, df_test], axis=1)
    df_all.to_csv(final_output_path, index=False)
    print(f"ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_output_path}")

def create_train_val_splits(full_dataset, n_splits=5, seed=42):
    """
    ë‹¨ì¼ CSV ê²½ë¡œë¥¼ ë°›ì•„ sklearnì˜ KFoldë¡œ 5ê°œ ë¶„í• ëœ (train, val) Subset íŠœí”Œì„ ë°˜í™˜
    """
    #full_dataset = SMILESDataset(csv_file=dataset_path, attn_bias_w=1.0, target_type="ex_prob")
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)

    train_val_splits = []
    for fold, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):
        train_subset = Subset(full_dataset, train_idx)
        val_subset = Subset(full_dataset, val_idx)
        train_val_splits.append((train_subset, val_subset))

    return train_val_splits

def cross_validate_model(
    config,
    target_type="ex_prob",
    loss_function_1="MSE",
    loss_function_2="MSE",
    weight_ex=0.5,
    num_epochs=10,
    batch_size=50,
    n_pairs=50,
    learning_rate=0.001,
    patience=20,
    alpha=0.12,
    train_dataset=None,
    test_dataset=None,
    cv_result_root = None,
    training_result_root = None,
    flattend_result_root = None,
    n_splits=5,
    use_ddp=True
):

    if train_dataset is None:
        train_dataset = SMILESDataset(csv_file="../../data/train_50.csv", attn_bias_w=1.0, target_type=target_type)
    if test_dataset is None:
        test_dataset = SMILESDataset(csv_file="../../data/test_100.csv", attn_bias_w=1.0, target_type=target_type)

    if cv_result_root is None:
        cv_result_root = "cv_results.csv"
    if training_result_root is None:
        training_result_root = "final_train_results.csv"
    if flattend_result_root is None:
        flattend_result_root = "flattened_final_results.csv"
    train_val_splits = create_train_val_splits(full_dataset=train_dataset, n_splits=n_splits)

    cross_validate_with_splits(
        train_val_splits=train_val_splits,
        config=config,
        target_type=target_type,
        loss_function_ex=loss_function_1,
        loss_function_prob=loss_function_2,
        num_epochs=num_epochs,
        batch_size=batch_size,
        n_pairs=n_pairs,
        learning_rate=learning_rate,
        patience=patience,
        alpha=alpha,
        training_result_root=cv_result_root,
        use_ddp=use_ddp
    )
    print("CV_finished")

    train_on_full_and_eval(
        config=config,
        full_train_dataset=train_dataset,
        test_dataset=test_dataset,
        target_type=target_type,
        loss_function_ex=loss_function_1,
        loss_function_prob=loss_function_2,
        num_epochs=num_epochs,
        batch_size=batch_size,
        n_pairs=n_pairs,
        learning_rate=learning_rate,
        patience=patience,
        alpha=alpha,
        training_result_root=training_result_root,
        use_ddp=use_ddp
    )
    print("training finished")

    cv_df = pd.read_csv(cv_result_root).iloc[:5]
    test_row = pd.read_csv(training_result_root).iloc[0]

    final_flattened_df = flatten_cv_and_train_test(cv_df, test_row)
    final_flattened_df.to_csv(flattend_result_root, index=False)
    if len(final_flattened_df) == 1:  # DataFrameì— í–‰ì´ í•œ ê°œì¼ ê²½ìš°
        final_flattened_dict = final_flattened_df.iloc[0].to_dict()  # ë‹¨ì¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    else:  # ì—¬ëŸ¬ í–‰ì´ ìˆì„ ê²½ìš°
        final_flattened_dict = final_flattened_df.to_dict(orient="records")  # ë¦¬ìŠ¤íŠ¸ ë‚´ ë”•ì…”ë„ˆë¦¬
    return final_flattened_dict

if __name__ == "__main__":
    dataset_path = "../../data/train_50.csv"
    target_type = "ex_prob"
    # ìë™ ë¶„í•  (csv í•œ ê°œì—ì„œ 5fold Subset ìƒì„±)
    config = {
        "num_atoms": 100,  # ë¶„ìì˜ ìµœëŒ€ ì›ì ìˆ˜ (ê·¸ë˜í”„ì˜ ë…¸ë“œ ê°œìˆ˜) 100
        "num_in_degree": 10,  # ê·¸ë˜í”„ ë…¸ë“œì˜ ìµœëŒ€ in-degree
        "num_out_degree": 10,  # ê·¸ë˜í”„ ë…¸ë“œì˜ ìµœëŒ€ out-degree
        "num_edges": 100,  # ê·¸ë˜í”„ì˜ ìµœëŒ€ ì—£ì§€ ê°œìˆ˜ 50
        "num_spatial": 100,  # ê³µê°„ì  ìœ„ì¹˜ ì¸ì½”ë”©ì„ ìœ„í•œ ìµœëŒ€ ê°’ default 100
        "num_edge_dis": 10,  # ì—£ì§€ ê±°ë¦¬ ì¸ì½”ë”©ì„ ìœ„í•œ ìµœëŒ€ ê°’
        "edge_type": "multi_hop",  # ì—£ì§€ íƒ€ì… ("multi_hop" ë˜ëŠ” ë‹¤ë¥¸ ê°’ ê°€ëŠ¥)
        "multi_hop_max_dist": 2,  # Multi-hop ì—£ì§€ì˜ ìµœëŒ€ ê±°ë¦¬
        "num_encoder_layers": 1,  # Graphormer ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ì¸ì½”ë” ë ˆì´ì–´ ê°œìˆ˜
        "embedding_dim": 128,  # ì„ë² ë”© ì°¨ì› í¬ê¸° (ë…¸ë“œ, ì—£ì§€ ë“±)
        "ffn_embedding_dim": 256,  # Feedforward Networkì˜ ì„ë² ë”© í¬ê¸°
        "num_attention_heads": 8,  # Multi-head Attentionì—ì„œ í—¤ë“œ ê°œìˆ˜
        "dropout": 0.1,  # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        "attention_dropout": 0.1,  # Attention ë ˆì´ì–´ì˜ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        "activation_dropout": 0.1,  # í™œì„±í™” í•¨ìˆ˜ ì´í›„ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        "activation_fn": "gelu",  # í™œì„±í™” í•¨ìˆ˜ ("gelu", "relu" ë“±)
        "pre_layernorm": False,  # LayerNormì„ Pre-Normalizationìœ¼ë¡œ ì‚¬ìš©í• ì§€ ì—¬ë¶€
        "q_noise": 0.0,  # Quantization noise (í›ˆë ¨ ì¤‘ ë…¸ì´ì¦ˆ ì¶”ê°€ë¥¼ ìœ„í•œ ë§¤ê°œë³€ìˆ˜)
        "qn_block_size": 8,  # Quantization block í¬ê¸°
        "output_size": 100,  # ëª¨ë¸ ì¶œë ¥ í¬ê¸°
    }

    train_dataset = SMILESDataset(csv_file=dataset_path, attn_bias_w=1.0, target_type=target_type)
    test_dataset = SMILESDataset(csv_file="../../data/test_100.csv", attn_bias_w=1.0, target_type=target_type)
    df = cross_validate_model(
        config,
        target_type="ex_prob",
        loss_function_1="SID",
        loss_function_2="SID",
        weight_ex=0.5,
        num_epochs=200,
        batch_size=50,
        n_pairs=50,
        learning_rate=0.001,
        patience=20,
        alpha=0.12,
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        cv_result_root = None,
        training_result_root = None,
        flattend_result_root = None,
    )
    print(df)






