import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import time
import wandb
import csv
import os

from src.models.transformer_model import GraphTransformer
from src.diffusion.noise_schedule import DiscreteUniformTransition, PredefinedNoiseScheduleDiscrete,\
    MarginalUniformTransition
from src.diffusion import diffusion_utils
from src.metrics.train_metrics import TrainLossDiscrete
from src.metrics.abstract_metrics import SumExceptBatchMetric, SumExceptBatchKL, NLL
from src import utils

def log_to_csv(log_dict, file_path='logs/val_metrics.csv'):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    file_exists = os.path.isfile(file_path)
    with open(file_path, 'a', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=log_dict.keys())
        if not file_exists:
            writer.writeheader()
        writer.writerow(log_dict)


class DiscreteDenoisingDiffusion(pl.LightningModule):
    def __init__(self, cfg, dataset_infos, train_metrics, sampling_metrics, visualization_tools, extra_features,
                 domain_features):
        super().__init__()
        ### dataset ì •ë³´ ê°€ì ¸ì˜¤ê¸° ###
        input_dims = dataset_infos.input_dims
        output_dims = dataset_infos.output_dims
        nodes_dist = dataset_infos.nodes_dist # ë…¸ë“œ ê°œìˆ˜ì— ëŒ€í•œ ì´ì‚° í™•ë¥  ë¶„í¬(discrete distribution)

        ### ì£¼ìš” ì„¤ì •ê°’ ###
        self.cfg = cfg # ëª¨ë¸ì˜ ì£¼ìš” ì„¤ì • ê°’ config íŒŒì¼ ì €ì¥
        self.name = cfg.general.name # í˜„ì¬ ëª¨ë¸ ì‹¤í—˜ ì´ë¦„
        self.model_dtype = torch.float32 # í…ì„œ íƒ€ì…
        self.T = cfg.model.diffusion_steps # diffusion stp ìˆ˜

        ### ë…¸ë“œ ë° ì—£ì§€, ê·¸ë˜í”„ íŠ¹ì§•ë“¤ ###
        self.Xdim = input_dims['X'] # ë…¸ë“œ
        self.Edim = input_dims['E'] # ì—£ì§€
        self.ydim = input_dims['y'] # ê·¸ë˜í”„ ì „ì—­
        self.Xdim_output = output_dims['X']
        self.Edim_output = output_dims['E']
        self.ydim_output = output_dims['y']
        self.node_dist = nodes_dist # ë…¸ë“œ ê°œìˆ˜ì— ëŒ€í•œ ì´ì‚° í™•ë¥  ë¶„í¬(discrete distribution)

        self.dataset_info = dataset_infos

        ### í•™ìŠµ ì¤‘ ì‚¬ìš©í•  ì†ì‹¤ í•¨ìˆ˜ ê°ì²´ë¥¼ ì´ˆê¸°í™” ###
        self.train_loss = TrainLossDiscrete(self.cfg.model.lambda_train)

        ## KL Divergence Loss , log ##
        # src.metrics.abstract_metrics í™•ì¸í•˜ê¸°
        self.val_nll = NLL()
        self.val_X_kl = SumExceptBatchKL()
        self.val_E_kl = SumExceptBatchKL()
        self.val_y_kl = SumExceptBatchKL()
        self.val_X_logp = SumExceptBatchMetric()
        self.val_E_logp = SumExceptBatchMetric()
        self.val_y_logp = SumExceptBatchMetric()

        self.test_nll = NLL()
        self.test_X_kl = SumExceptBatchKL()
        self.test_E_kl = SumExceptBatchKL()
        self.test_y_kl = SumExceptBatchKL()
        self.test_X_logp = SumExceptBatchMetric()
        self.test_E_logp = SumExceptBatchMetric()
        self.test_y_logp = SumExceptBatchMetric()

        ### í•™ìŠµ ì¤‘ ì‚¬ìš©í•˜ëŠ” ì •ëŸ‰ í‰ê°€ ì§€í‘œ ê³„ì‚° ë„êµ¬###
        # ì¼ë°˜ì ìœ¼ë¡œ ë…¸ë“œ/ì—£ì§€ ì˜ˆì¸¡ ì •í™•ë„, loss, KL divergence ë“±ì„ ë¡œê·¸ë¡œ ë‚¨ê¹ë‹ˆë‹¤.
        # ë‚´ë¶€ì ìœ¼ë¡œ TrainMolecularMetricsDiscrete í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        self.train_metrics = train_metrics

        ### ìƒ˜í”Œë§ëœ ë¶„ìë“¤ì— ëŒ€í•´ í™”í•™ì ìœ¼ë¡œ ìœ íš¨í•œ ë¶„ìì¸ì§€ í‰ê°€í•˜ëŠ” ë©”íŠ¸ë¦­ ê³„ì‚°ê¸°ì…ë‹ˆë‹¤.
        #ìœ íš¨ì„±(validity), ê³ ìœ ì„±(uniqueness), ë‹¤ì–‘ì„±(diversity), novelty ë“±ì˜ ì§€í‘œ ê³„ì‚°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
        #ë³´í†µ sampling í›„ self.sampling_metrics(samples) í˜•íƒœë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        self.sampling_metrics = sampling_metrics

        ### ìƒì„±ëœ ë¶„ì êµ¬ì¡°ë¥¼ ì´ë¯¸ì§€ë¡œ ì‹œê°í™” ###
        # src/analysis/visualization.pyì— ì •ì˜ë˜ì–´ ìˆìŒ
        self.visualization_tools = visualization_tools

        ### ë…¸ë“œ/ì—£ì§€/ì „ì—­ íŠ¹ì„± ì™¸ì— ì¶”ê°€ì ìœ¼ë¡œ ì…ë ¥ë˜ëŠ” MLìš© feature ìƒì„±ê¸° ###
        self.extra_features = extra_features

        ### í™”í•™ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ì˜ íŠ¹í™”ëœ feature ìƒì„± ë„êµ¬ ###
        self.domain_features = domain_features

        ### denoisingì„ ìœ„í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ ###
        self.model = GraphTransformer(n_layers=cfg.model.n_layers, # ëª‡ê°œì˜ Trasnformer Layer ì‚¬ìš©í•  ê²ƒì¸ì§€
                                      input_dims=input_dims, # ì…ë ¥ ì°¨ì› Node, Edge, Graphì „ì—­ Y
                                      hidden_mlp_dims=cfg.model.hidden_mlp_dims, #MLP ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì°¨ì›
                                      hidden_dims=cfg.model.hidden_dims, # Transformer ë‚´ hidden ì°¨ì›
                                      output_dims=output_dims, # ì¶œë ¥ ì°¨ì› Node, Edge, Graphì „ì—­ Y
                                      act_fn_in=nn.ReLU(), # ì…ë ¥ìš© activation function
                                      act_fn_out=nn.ReLU()) # ì¶œë ¥ìš© activation function

        ### Discrete diffusion ê³¼ì •ì—ì„œ timestepë³„ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„(Î², Î±Ì„ ë“±)ì„ ì •ì˜ ###
        self.noise_schedule = PredefinedNoiseScheduleDiscrete(cfg.model.diffusion_noise_schedule,
                                                              timesteps=cfg.model.diffusion_steps)

        ### ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´ **ë™ì¼í•œ í™•ë¥ (ê· ë“±)**ë¡œ ë…¸ì´ì¦ˆê°€ ë¶„í¬ëœë‹¤ê³  ê°€ì • ###
        if cfg.model.transition == 'uniform':
            self.transition_model = DiscreteUniformTransition(x_classes=self.Xdim_output, e_classes=self.Edim_output,
                                                              y_classes=self.ydim_output)
            # ë…¸ë“œ type ì´ 5ê°œë‹¤ CNOSP -> Cì¼ í™•ë¥ ì€ 1/5ë¡œ ì„¤ì •ë¨
            x_limit = torch.ones(self.Xdim_output) / self.Xdim_output
            e_limit = torch.ones(self.Edim_output) / self.Edim_output
            y_limit = torch.ones(self.ydim_output) / self.ydim_output
            self.limit_dist = utils.PlaceHolder(X=x_limit, E=e_limit, y=y_limit)

        ### ì‹¤ì œ ë°ì´í„°ì…‹ì˜ node/edge class ë¹ˆë„ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥  ë¶„í¬ ìƒì„±###
        elif cfg.model.transition == 'marginal':
            node_types = self.dataset_info.node_types.float()
            x_marginals = node_types / torch.sum(node_types) # node type ë¹„ìœ¨

            edge_types = self.dataset_info.edge_types.float()
            e_marginals = edge_types / torch.sum(edge_types) # edge type ë¹„ìœ¨
            print(f"Marginal distribution of the classes: {x_marginals} for nodes, {e_marginals} for edges")

            # ìœ„ì—ì„œ ê³„ì‚°í•œ ì‹¤ ë°ì´í„° ê¸°ë°˜ í™•ë¥  ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì´ ëª¨ë¸ (transition model) ì„¤ì •
            self.transition_model = MarginalUniformTransition(x_marginals=x_marginals, e_marginals=e_marginals,
                                                              y_classes=self.ydim_output)

            # limit_dist ì„¤ì •
            # diffusion ê³¼ì •ì˜ ìµœì¢… ë‹¨ê³„ ğ‘¡=ğ‘‡ ì— ë„ë‹¬í–ˆì„ ë•Œ, ë°ì´í„°ê°€ ìˆ˜ë ´í•´ì•¼ í•˜ëŠ” í™•ë¥  ë¶„í¬
            # PlaceHolder : x_marginals, e_marginals, torch.ones(...) / ... â† ì´ ì„¸ ê°œì˜ ë¶„í¬ë¥¼ í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ ë‹¤ë£¨ê¸°
            self.limit_dist = utils.PlaceHolder(X=x_marginals, E=e_marginals,
                                                y=torch.ones(self.ydim_output) / self.ydim_output)


        self.save_hyperparameters(ignore=[train_metrics, sampling_metrics]) # PyTorch Lightningì—ì„œ ëª¨ë¸ ì´ˆê¸°í™” ì‹œ ì „ë‹¬ëœ ëª¨ë“  ì¸ì(config, kwargs ë“±)ë¥¼ ìë™ ì €ì¥
        self.start_epoch_time = None # ê° epoch ì‹œì‘ ì‹œ ì‹œê°„ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        self.train_iterations = None # í•œ epochì— train/val ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí•˜ëŠ”ì§€ (iteration ìˆ˜)
        self.val_iterations = None
        self.log_every_steps = cfg.general.log_every_steps  # ëª‡ stepë§ˆë‹¤ ë¡œê·¸ë¥¼ ì¶œë ¥í• ì§€ ì„¤ì •
        self.number_chain_steps = cfg.general.number_chain_steps # diffusion ìƒ˜í”Œë§ ê³¼ì •ì—ì„œ ëª‡ ë‹¨ê³„ì˜ ì¤‘ê°„ ê²°ê³¼(chain)ë¥¼ ì €ì¥í• ì§€
        self.best_val_nll = 1e8 # í˜„ì¬ê¹Œì§€ ê´€ì¸¡ëœ ìµœì†Œ validation NLL (Negative Log Likelihood) ë¥¼ ì €ì¥
        self.val_counter = 0 # validation epoch íšŸìˆ˜ ì¶”ì ìš© ì¹´ìš´í„°

    def training_step(self, data, i):

        ### PyGì˜ sparse í˜•ì‹ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ dense í…ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ###
        # ë…¸ë“œ feature xë¥¼ (B, N, F)ë¡œ ë³€í™˜, ê°€ì¥ í° ê·¸ë˜í”„ì— ë§ì¶°ì„œ ì—¬ëŸ¬ ê·¸ë˜í”„ë¥¼ íŒ¨ë”© + node_mask ìƒì„±
        # ìê¸° ìì‹ ì— ëŒ€í•œ ì—£ì§€ë¥¼ ì œê±°, edge_index -> ì¸ì ‘í–‰ë ¬, ì—£ì§€ê°€ ì—†ëŠ” ìœ„ì¹˜ë¥¼ "no-edge í´ë˜ìŠ¤"ë¡œ í‘œì‹œ
        # ë…¸ë“œ/ì—£ì§€ í…ì„œë¥¼ í•˜ë‚˜ë¡œ ë¬¶ìŒ
        dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)

        dense_data = dense_data.mask(node_mask) # ìœ íš¨í•œ ë…¸ë“œë§Œ êµ¬ë¶„í•˜ê¸° ìœ„í•œ binary mask

        # ë…¸ì´ì¦ˆ ì£¼ì… (forward diffusion ë‹¨ê³„)
        X, E = dense_data.X, dense_data.E
        noisy_data = self.apply_noise(X, E, data.y, node_mask) # --> í•¨ìˆ˜í™•ì¸í•˜ê¸°

        # extra_features, domain_features, t ë“±ì„ ê³„ì‚°í•˜ì—¬ ì…ë ¥ì— ì¶”ê°€ # ì•„ë˜ìª½ ë¼ì¸ í™•ì¸
        extra_data = self.compute_extra_data(noisy_data)

        # ëª¨ë¸ ì˜ˆì¸¡
        pred = self.forward(noisy_data, extra_data, node_mask)

        # ì†ì‹¤ ê³„ì‚°
        loss = self.train_loss(masked_pred_X=pred.X, masked_pred_E=pred.E, pred_y=pred.y,
                               true_X=X, true_E=E, true_y=data.y,
                               log=i % self.log_every_steps == 0)

        # í•™ìŠµ ë©”íŠ¸ë¦­ ê¸°ë¡
        self.train_metrics(masked_pred_X=pred.X, masked_pred_E=pred.E, true_X=X, true_E=E,
                           log=i % self.log_every_steps == 0)

        return {'loss': loss}

    # í•™ìŠµì— ì‚¬ìš©í•  ì˜µí‹°ë§ˆì´ì €ë¥¼ ì •ì˜ (ì—¬ê¸°ì„œëŠ” AdamW)
    def configure_optimizers(self):
        """ self.cfg.train.lrê³¼ weight_decayëŠ” ì„¤ì • íŒŒì¼ì—ì„œ ì§€ì • """
        return torch.optim.AdamW(self.parameters(), lr=self.cfg.train.lr, amsgrad=True,
                                 weight_decay=self.cfg.train.weight_decay)
    # ì „ì²´ í•™ìŠµ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë¨
    # train ë°ì´í„°ì˜ ì „ì²´ ë°˜ë³µ ìˆ˜ ê³„ì‚° (í•œ epoch ë‹¹ step ìˆ˜)
    def on_fit_start(self) -> None:
        self.train_iterations = len(self.trainer.datamodule.train_dataloader())
        print("Size of the input features", self.Xdim, self.Edim, self.ydim)

    # ë§¤ epoch ì‹œì‘ ì‹œ í˜¸ì¶œ
    # ì‹œê°„ ì¸¡ì • ì‹œì‘ (start_epoch_time)
    # í•™ìŠµ ì†ì‹¤ ê°ì²´ ë° ë©”íŠ¸ë¦­ ê°ì²´ ì´ˆê¸°í™”
    def on_train_epoch_start(self) -> None:
        print("Starting train epoch...")
        self.start_epoch_time = time.time()
        self.train_loss.reset()
        self.train_metrics.reset()

    # ë§¤ epoch ì¢…ë£Œ ì‹œ í˜¸ì¶œ
    # ì†ì‹¤/ì •í™•ë„ ë“±ì˜ epoch ë‹¨ìœ„ ë¡œê·¸ ê¸°ë¡ (wandb.log() ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ë  ê°€ëŠ¥ì„± ë†’ìŒ)
    # ê²½ê³¼ ì‹œê°„ í¬í•¨
    def on_train_epoch_end(self) -> None:
        self.train_loss.log_epoch_metrics(self.current_epoch, self.start_epoch_time)
        self.train_metrics.log_epoch_metrics(self.current_epoch)

    # validation epoch ì‹œì‘ ì „ í˜¸ì¶œ
    # validationì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ë©”íŠ¸ë¦­ ê°ì²´ ì´ˆê¸°í™”
    # ì˜ˆ: NLL, KL divergence, log-likelihood, ìƒ˜í”Œë§ ê´€ë ¨ ë©”íŠ¸ë¦­ ë“±
    def on_validation_epoch_start(self) -> None:
        self.val_nll.reset()
        self.val_X_kl.reset()
        self.val_E_kl.reset()
        self.val_y_kl.reset()
        self.val_X_logp.reset()
        self.val_E_logp.reset()
        self.val_y_logp.reset()
        self.sampling_metrics.reset()


    def validation_step(self, data, i):
        dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch) # PyGì˜ sparse í˜•ì‹ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ dense í…ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        dense_data = dense_data.mask(node_mask)

        noisy_data = self.apply_noise(dense_data.X, dense_data.E, data.y, node_mask) # ë…¸ì´ì¦ˆ ì£¼ì… (forward diffusion ë‹¨ê³„)
        extra_data = self.compute_extra_data(noisy_data) # extra_features, domain_features, t ë“±ì„ ê³„ì‚°í•˜ì—¬ ì…ë ¥ì— ì¶”ê°€

        pred = self.forward(noisy_data, extra_data, node_mask) # ëª¨ë¸ ì˜ˆì¸¡
        nll = self.compute_val_loss(pred, noisy_data, dense_data.X, dense_data.E, data.y,  node_mask, test=False) # ì†ì‹¤ ê³„ì‚°
        return {'loss': nll}

    # validation epoch ì‹œì‘ í›„ í˜¸ì¶œ
    #def validation_epoch_end(self, outs) -> None:
    def on_validation_epoch_end(self) -> None:

        # Metric ê³„ì‚° ë° ë¡œê·¸ ì¶œë ¥
        # nll : Negative Log-Likelihood
        metrics = [self.val_nll.compute(), self.val_X_kl.compute(), self.val_E_kl.compute(),
                   self.val_y_kl.compute(), self.val_X_logp.compute(), self.val_E_logp.compute(),
                   self.val_y_logp.compute()]
        print("validation metrics:", metrics)
        metrics = {
            "epoch": self.current_epoch,
            "val_nll": metrics[0].item(),
            "val_X_kl": metrics[1].item(),
            "val_E_kl": metrics[2].item(),
            "val_y_kl": metrics[3].item(),
            "val_X_logp": metrics[4].item(),
            "val_E_logp": metrics[5].item(),
            "val_y_logp": metrics[6].item()
        }
        print("Validation metrics:", metrics)
        log_to_csv(metrics, file_path='logs/val_metrics.csv')

        val_nll_key = "val_nll"
        val_X_kl_key = f"val_X_kl"
        val_E_kl_key = f"val_E_kl"
        val_y_kl_key = f"val_y_kl"
        print(f"Epoch {self.current_epoch}: Val NLL {metrics[val_nll_key] :.2f} -- Val Atom type KL {metrics[val_X_kl_key] :.2f} -- ",
              f"Val Edge type KL: {metrics[val_E_kl_key] :.2f} -- Val Global feat. KL {metrics[val_y_kl_key] :.2f}\n")

        # Log val nll with default Lightning logger, so it can be monitored by checkpoint callback
        # Checkpoint í‰ê°€ ê¸°ì¤€ ê¸°ë¡
        # ê¶ê¸ˆ í•  ì‹œ main.py ì•ˆ checkpoint_callback = ModelCheckpoint í™•ì¸í•´ ë³´ê¸°,
        val_nll = metrics[val_nll_key]

        # PyTorch Lightning ë‚´ë¶€ loggerì— "val/epoch_NLL"ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ê°’ ì €ì¥ | callbackë“¤ì´ ì½ì„ ìˆ˜ ìˆëŠ” í•µì‹¬ ë¡œê·¸ í‚¤(key)
        self.log("val/epoch_NLL", val_nll)

        # callbackì„ í†µí•´ val ì´ nll ê¸°ì¡´ ë³´ë‹¤ ë‚®ì„ ê²½ìš° best_val_nllì— ì €ì¥
        if val_nll < self.best_val_nll:
            self.best_val_nll = val_nll
        print('Val loss: %.4f \t Best val loss:  %.4f\n' % (val_nll, self.best_val_nll))

        ## ìƒ˜í”Œ ìƒì„± ì¡°ê±´ í™•ì¸ ë° ì‹¤í–‰ ##
        self.val_counter += 1
        # ë§¤ sample_every_valë²ˆ validation epochë§ˆë‹¤ í•œ ë²ˆ
        if self.val_counter % self.cfg.general.sample_every_val == 0:
            start = time.time()

            ## ìƒ˜í”Œ ìƒì„± ê°œìˆ˜ ì„¤ì • config íŒŒì¼ ì½ê¸°##
            samples_left_to_generate = self.cfg.general.samples_to_generate # ì–¼ë§ˆë‚˜ ë§ì€ ìƒ˜í”Œì„ ë§Œë“¤ ê²ƒì¸ì§€
            samples_left_to_save = self.cfg.general.samples_to_save # ê·¸ì¤‘ ì €ì¥í•  ìƒ˜í”Œ ìˆ˜
            chains_left_to_save = self.cfg.general.chains_to_save # sampling ê³¼ì •(chain) ì €ì¥í•  ìˆ˜ ë“± ì„¤ì •
            samples = []

            ident = 0
            ## ìƒ˜í”Œ ìƒì„± ë£¨í”„ ##
            while samples_left_to_generate > 0: # ì´ samples_to_generate ë§Œí¼ ìƒì„±ë  ë•Œê¹Œì§€ ë°˜ë³µ
                bs = 2 * self.cfg.train.batch_size # í•œ ë²ˆì— ìµœëŒ€ 2 Ã— batch_size ë§Œí¼ ìƒì„±
                to_generate = min(samples_left_to_generate, bs) # ë‘ ë³€ìˆ˜ì¤‘ ì‘ì€ ìˆ˜ ë§Œí¼ ìƒì„±ë˜ë„ë¡ ë³€ìˆ˜ì§€ì •
                to_save = min(samples_left_to_save, bs) # ë‘ ë³€ìˆ˜ì¤‘ ì‘ìœ¼ ìˆ˜ë§Œí¼ ì €ì¥ë˜ë„ë¡ ë³€ìˆ˜ì§€ì •
                chains_save = min(chains_left_to_save, bs) # ë‘ ë³€ìˆ˜ì¤‘ ì‘ì€ ìˆ˜ë§Œí¼ chains (ìƒ˜í”Œë§ ì¤‘ê°„ê³¼ì •)ê°€ ì €ì¥ë¨

                # ì‹¤ì œ ìƒ˜í”Œ ìƒì„±ì€ self.sample_batch(...) í•¨ìˆ˜ì—ì„œ ìˆ˜í–‰
                samples.extend(self.sample_batch(batch_id=ident, batch_size=to_generate, num_nodes=None,
                                                 save_final=to_save,
                                                 keep_chain=chains_save,
                                                 number_chain_steps=self.number_chain_steps))

                # count ì— ëŒ€í•œ ë³€í™”
                ident += to_generate

                samples_left_to_save -= to_save
                samples_left_to_generate -= to_generate
                chains_left_to_save -= chains_save

            # ìƒì„±ëœ ìƒ˜í”Œ í‰ê°€
            print("Computing sampling metrics...")
            self.sampling_metrics(samples, self.name, self.current_epoch, val_counter=-1, test=False)
            print(f'Done. Sampling took {time.time() - start:.2f} seconds\n')
            self.sampling_metrics.reset()

    # í…ŒìŠ¤íŠ¸ ì‹œì‘ ì „ì— ëª¨ë“  metricì„ ì´ˆê¸°í™”í•¨
    def on_test_epoch_start(self) -> None:
        self.test_nll.reset()
        self.test_X_kl.reset()
        self.test_E_kl.reset()
        self.test_y_kl.reset()
        self.test_X_logp.reset()
        self.test_E_logp.reset()
        self.test_y_logp.reset()

    # í…ŒìŠ¤íŠ¸ìš© í•œ ë°°ì¹˜ì—ì„œì˜ forward â†’ loss ê³„ì‚°
    def test_step(self, data, i):
        dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch) # PyGì˜ sparse í˜•ì‹ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ dense í…ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        dense_data = dense_data.mask(node_mask)
        noisy_data = self.apply_noise(dense_data.X, dense_data.E, data.y, node_mask) # ë…¸ì´ì¦ˆ ì£¼ì… (forward diffusion ë‹¨ê³„)
        extra_data = self.compute_extra_data(noisy_data) # extra_features, domain_features, t ë“±ì„ ê³„ì‚°í•˜ì—¬ ì…ë ¥ì— ì¶”ê°€
        pred = self.forward(noisy_data, extra_data, node_mask) # ì˜ˆì¸¡ ìˆ˜í–‰
        nll = self.compute_val_loss(pred, noisy_data, dense_data.X, dense_data.E, data.y, node_mask, test=True) # loss ê³„ì‚°,val ì—ì„œëŠ” true ì˜€ìŒ
        return {'loss': nll}

    def test_epoch_end(self, outs) -> None:
        """ Measure likelihood on a test set and compute stability metrics. """
        ## Metric ê³„ì‚° ë° ë¡œê·¸ ê¸°ë¡##
        # ê° í…ŒìŠ¤íŠ¸ ì§€í‘œ (NLL, KL, logP ë“±)ë¥¼ ê³„ì‚°í•˜ê³  wandbì— ê¸°ë¡
        metrics = [self.test_nll.compute(), self.test_X_kl.compute(), self.test_E_kl.compute(),
                   self.test_y_kl.compute(), self.test_X_logp.compute(), self.test_E_logp.compute(),
                   self.test_y_logp.compute()]

        metrics = {
            "epoch": self.current_epoch,
            "test_nll": metrics[0].item(),
            "test_X_kl": metrics[1].item(),
            "test_E_kl": metrics[2].item(),
            "test_y_kl": metrics[3].item(),
            "test_X_logp": metrics[4].item(),
            "test_E_logp": metrics[5].item(),
            "test_y_logp": metrics[6].item()
        }
        test_nll_key = "test_nll"
        test_X_kl_key = "test_X_kl"
        test_E_kl_key = "test_E_kl"
        test_y_kl_key = "test_y_kl"

        print(f"Epoch {self.current_epoch}: Test NLL {metrics[test_nll_key] :.2f} -- Test Atom type KL {metrics[test_X_kl_key] :.2f} -- ",
              f"Test Edge type KL: {metrics[test_E_kl_key] :.2f} -- Test Global feat. KL {metrics[test_y_kl_key] :.2f}\n")

        test_nll = metrics[0]
        #wandb.log({"test/epoch_NLL": test_nll}, commit=False)
        print(f'Test loss: {test_nll :.4f}')

        # configuration íŒŒì¼ì—ì„œ ì–¼ë§ˆë‚˜ ìƒ˜í”Œ ìƒì„±í• ì§€ #
        samples_left_to_generate = self.cfg.general.final_model_samples_to_generate # ì–¼ë§ˆë‚˜ ë§ì€ ìƒ˜í”Œì„ ë§Œë“¤ ê²ƒì¸ì§€
        samples_left_to_save = self.cfg.general.final_model_samples_to_save  # ê·¸ì¤‘ ì €ì¥í•  ìƒ˜í”Œ ìˆ˜
        chains_left_to_save = self.cfg.general.final_model_chains_to_save # sampling ê³¼ì •(chain) ì €ì¥í•  ìˆ˜ ë“± ì„¤ì •

        samples = []

        ## ìƒ˜í”Œ ìƒì„± ë£¨í”„ ##
        id = 0
        while samples_left_to_generate > 0:
            print(f'Samples left to generate: {samples_left_to_generate}/'
                  f'{self.cfg.general.final_model_samples_to_generate}', end='', flush=True)
            bs = 2 * self.cfg.train.batch_size # í•œë²ˆì— batch size * 2 ë§Œí¼ ìƒì„±
            to_generate = min(samples_left_to_generate, bs)
            to_save = min(samples_left_to_save, bs)
            chains_save = min(chains_left_to_save, bs)
            samples.extend(self.sample_batch(id, to_generate, num_nodes=None, save_final=to_save,
                                             keep_chain=chains_save, number_chain_steps=self.number_chain_steps))
            id += to_generate
            samples_left_to_save -= to_save
            samples_left_to_generate -= to_generate
            chains_left_to_save -= chains_save

        # ìƒ˜í”Œë§ ì„±ëŠ¥ í‰ê°€
        # validity (í™”í•™ì ìœ¼ë¡œ ìœ íš¨í•œì§€)
        # uniqueness (ì¤‘ë³µ ì—†ëŠ”ì§€)
        # novelty (í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ìƒˆë¡œìš´ êµ¬ì¡°ì¸ì§€)
        # diversity, FCD ë“±
        print("Computing sampling metrics...")
        self.sampling_metrics.reset()
        self.sampling_metrics(samples, self.name, self.current_epoch, self.val_counter, test=True)
        self.sampling_metrics.reset()
        print("Done.")

    ##### "ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ diffusionì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ T ë…¸ì´ì¦ˆ ë¶„í¬" ì™€ "ë¯¸ë¦¬ ë°ì´í„°ì…‹ì—ì„œ ë§Œë“¤ì–´ì§„ ì‚¬ì „ ë¶„í¬" ì‚¬ì´ì—ì„œ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ì§€ë¥¼ ì¸¡ì • #####
    def kl_prior(self, X, E, y, node_mask): # compute_val_lossì—ì„œ ì‚¬ìš©ë¨
        """Computes the KL between q(z1 | x) and the prior p(z1) = Normal(0, 1).

        This is essentially a lot of work for something that is in practice negligible in the loss. However, you
        compute it so that you see it when you've made a mistake in your noise schedule.
        """


        ### Compute the last alpha value, alpha_T.
        ones = torch.ones((X.size(0), 1), device=X.device) # 1ë¡œ ì±„ì›Œì§„ í…ì„œ (bs, 1)
        Ts = self.T * ones # "1ë¡œ ì±„ì›Œì§„ í…ì„œ"ì— "ì „ì²´ diffusion step ìˆ˜" ê³±í•˜ê¸°
        alpha_t_bar = self.noise_schedule.get_alpha_bar(t_int=Ts)  # (bs, 1) , PredefinedNoiseScheduleDiscrete, step 1ë¶€í„° tê¹Œì§€ ê³±í•´ì§„ê²ƒ, ì›ë³¸ ì •ë³´ê°€ ì–¼ë§ˆë‚˜ ìœ ì§€ë˜ì—ˆëŠ”ì§€

        ### í™•ë¥  ì „ì´ í–‰ë ¬ ìƒì„± Q t bar QÌ…_T = Qtb
        # DiscreteUniformTransition ë˜ëŠ” MarginalUniformTransition
        Qtb = self.transition_model.get_Qt_bar(alpha_t_bar, self.device) # transition matrix ìƒì„± | ê° ì…ë ¥ í´ë˜ìŠ¤ â†’ ê° ì¶œë ¥ í´ë˜ìŠ¤ ë¡œ ê°ˆ í™•ë¥  ë¶„í¬

        ### Compute transition probabilities
        # Qtb ê³±í•˜ì—¬ í™•ë¥  ë¶„í¬ ê³„ì‚° = "ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ diffusionì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ T ë…¸ì´ì¦ˆ ë¶„í¬"
        probX = X @ Qtb.X  # (bs, n, dx_out)
        probE = E @ Qtb.E.unsqueeze(1)  # (bs, n, n, de_out)
        proby = y @ Qtb.y if y.numel() > 0 else y
        assert probX.shape == X.shape

        bs, n, _ = probX.shape

        # limit_dist : Node ë° Edge Class ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ = "ë¯¸ë¦¬ ë°ì´í„°ì…‹ì—ì„œ ë§Œë“¤ì–´ì§„ ì‚¬ì „ ë¶„í¬"
        # ë…¸ë“œ í´ë˜ìŠ¤ë³„í™•ë¥ ë¶„í¬(C:0.2, O:0.8), ì—£ì§€ í´ë˜ìŠ¤ë³„ í™•ë¥ ë¶„í¬(ë‹¨ì¼:0.8, ì´ì¤‘:0.2) ì´ëŸ° í˜•íƒœë¥¼ probXë‚˜ probE ë“± ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì™€ ê°™ê²Œ ë§Œë“¤ê¸°
        limit_X = self.limit_dist.X[None, None, :].expand(bs, n, -1).type_as(probX)
        limit_E = self.limit_dist.E[None, None, None, :].expand(bs, n, n, -1).type_as(probE)
        uniform_dist_y = torch.ones_like(proby) / self.ydim_output

        ### Make sure that masked rows do not contribute to the loss
        # ë§ˆìŠ¤í¬ ì ìš© | paddingëœ node/edgeëŠ” KL ê³„ì‚°ì—ì„œ ì œì™¸
        limit_dist_X, limit_dist_E, probX, probE = diffusion_utils.mask_distributions(true_X=limit_X.clone(),
                                                                                      true_E=limit_E.clone(),
                                                                                      pred_X=probX,
                                                                                      pred_E=probE,
                                                                                      node_mask=node_mask)

        # probX = ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥  ë¶„í¬
        # limit_dist_X = ì‚¬ì „ ë¶„í¬
        kl_distance_X = F.kl_div(input=probX.log(), target=limit_dist_X, reduction='none')
        kl_distance_E = F.kl_div(input=probE.log(), target=limit_dist_E, reduction='none')
        kl_distance_y = F.kl_div(input=proby.log(), target=uniform_dist_y, reduction='none')

        return diffusion_utils.sum_except_batch(kl_distance_X) + \
               diffusion_utils.sum_except_batch(kl_distance_E) + \
               diffusion_utils.sum_except_batch(kl_distance_y)

    ### Diffusion ë‹¨ê³„ ì†ì‹¤ (ì „ì²´ ì‹œì  í†µí•©) ###
    # ëª¨ë¸ì´ ê° ì‹œì  tì—ì„œ z_t â†’ z_{t-1} ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€
    # ê° ìƒ˜í”Œë³„ë¡œ ì—¬ëŸ¬ tì—ì„œ í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„
    # ë‹¤ì‹œ í•œë²ˆ ë³´ê¸° ì´í•´ ì˜ x
    def compute_Lt(self, X, E, y, pred, noisy_data, node_mask, test): # compute_val_lossì—ì„œ ì‚¬ìš©ë¨
        ### ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ í™”
        # ëª¨ë¸ì˜ ì¶œë ¥ logits â†’ softmaxë¡œ í™•ë¥ ë¡œ ë³€í™˜
        pred_probs_X = F.softmax(pred.X, dim=-1) # node X features (ë§¤ìš° +ë‚˜ -ì¸ ìˆ˜ ê°€ëŠ¥) -> node X features (0~1)
        pred_probs_E = F.softmax(pred.E, dim=-1)
        pred_probs_y = F.softmax(pred.y, dim=-1)

        ### ì „ì´ í–‰ë ¬ ê³„ì‚°
        Qtb = self.transition_model.get_Qt_bar(noisy_data['alpha_t_bar'], self.device) # t ì‹œì ì˜ ì „ì´ í™•ë¥  = t ì‹œì ê¹Œì§€ì˜ Qt ê³±í•˜ì—¬ ë§Œë“  Qtb | (node X node)
        Qsb = self.transition_model.get_Qt_bar(noisy_data['alpha_s_bar'], self.device) # t-1 ì‹œì ì˜ ì „ì´ í™•ë¥  = t-1 ì‹œì ê¹Œì§€ì˜ Qt ê³±í•˜ì—¬ ë§Œë“  Qsb
        Qt = self.transition_model.get_Qt(noisy_data['beta_t'], self.device) # ì§ì ‘ì ì¸ forward transition = t -> t-1 ë¡œ ê°€ê¸°ìœ„í•œ Qt

        ### Compute distributions to compare with KL
        bs, n, d = X.shape
        # Ground truth ë¶„í¬ ê³„ì‚° : ëª¨ë¸ì´ ì˜ˆì¸¡í•´ì•¼ í•  ì •ë‹µ ë¶„í¬ # diffusion_utils.posterior_distributions í™•ì¸í•˜ê¸°
        prob_true = diffusion_utils.posterior_distributions(X=X, E=E, y=y, X_t=noisy_data['X_t'], E_t=noisy_data['E_t'],
                                                            y_t=noisy_data['y_t'], Qt=Qt, Qsb=Qsb, Qtb=Qtb)
        prob_true.E = prob_true.E.reshape((bs, n, n, -1))
        # ëª¨ë¸ì˜ ì¶œë ¥ìœ¼ë¡œ ë¶€í„° ì˜ˆì¸¡ ë¶„í¬ ê³„ì‚° # diffusion_utils.posterior_distributions í™•ì¸í•˜ê¸°
        prob_pred = diffusion_utils.posterior_distributions(X=pred_probs_X, E=pred_probs_E, y=pred_probs_y,
                                                            X_t=noisy_data['X_t'], E_t=noisy_data['E_t'],
                                                            y_t=noisy_data['y_t'], Qt=Qt, Qsb=Qsb, Qtb=Qtb)
        prob_pred.E = prob_pred.E.reshape((bs, n, n, -1))

        # Reshape and filter masked rows | ë§ˆìŠ¤í¬ ì ìš©, padding ëœ ë…¸ë“œ / ì—£ì§€ë¥¼ KL ê³„ì‚°ì—ì„œ ì œì™¸
        prob_true_X, prob_true_E, prob_pred.X, prob_pred.E = diffusion_utils.mask_distributions(true_X=prob_true.X,
                                                                                                true_E=prob_true.E,
                                                                                                pred_X=prob_pred.X,
                                                                                                pred_E=prob_pred.E,
                                                                                                node_mask=node_mask)
        # from src.metrics.abstract_metrics import SumExceptBatchKL
        kl_x = (self.test_X_kl if test else self.val_X_kl)(prob_true.X, torch.log(prob_pred.X))
        kl_e = (self.test_E_kl if test else self.val_E_kl)(prob_true.E, torch.log(prob_pred.E))
        kl_y = (self.test_y_kl if test else self.val_y_kl)(prob_true.y, torch.log(prob_pred.y)) if pred_probs_y.numel() != 0 else 0
        return kl_x + kl_e + kl_y

    def reconstruction_logp(self, t, X, E, y, node_mask): # compute_val_loss ì—ì„œ ì‚¬ìš©ë¨
        ### Compute noise values for t = 0. t=0ì¼ ë•Œì˜ transition matrix ê³„ì‚° ###
        t_zeros = torch.zeros_like(t)
        beta_0 = self.noise_schedule(t_zeros)
        Q0 = self.transition_model.get_Qt(beta_t=beta_0, device=self.device)

        # ì›ë˜ ì…ë ¥ X, Eì— Q0ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ë¶„í¬ ê³„ì‚° | ê° ë…¸ë“œ/ì—£ì§€ê°€ íŠ¹ì • í´ë˜ìŠ¤ì¼ í™•ë¥ 
        probX0 = X @ Q0.X  # (bs, n, dx_out)
        probE0 = E @ Q0.E.unsqueeze(1)  # (bs, n, n, de_out)

        # ì´ ë¶„í¬ë¡œë¶€í„° ìƒ˜í”Œë§í•˜ì—¬ X0, E0, y0 ìƒì„±
        sampled0 = diffusion_utils.sample_discrete_features(probX=probX0, probE=probE0, node_mask=node_mask)

        X0 = F.one_hot(sampled0.X, num_classes=self.Xdim_output).float()
        E0 = F.one_hot(sampled0.E, num_classes=self.Edim_output).float()
        y0 = sampled0.y
        assert (X.shape == X0.shape) and (E.shape == E0.shape)

        sampled_0 = utils.PlaceHolder(X=X0, E=E0, y=y0).mask(node_mask)

        ### Predictions | ì´ noisyí•œ ì…ë ¥ìœ¼ë¡œ ë‹¤ì‹œ prediction ìˆ˜í–‰ ###
        noisy_data = {'X_t': sampled_0.X, 'E_t': sampled_0.E, 'y_t': sampled_0.y, 'node_mask': node_mask,
                      't': torch.zeros(X0.shape[0], 1).type_as(y0)}
        extra_data = self.compute_extra_data(noisy_data)
        pred0 = self.forward(noisy_data, extra_data, node_mask)

        ### Normalize predictions
        probX0 = F.softmax(pred0.X, dim=-1)
        probE0 = F.softmax(pred0.E, dim=-1)
        proby0 = F.softmax(pred0.y, dim=-1)

        ### Set masked rows to arbitrary values that don't contribute to loss
        # ë§ˆìŠ¤í¬ë¡œ ë¹„ìœ íš¨í•œ ë…¸ë“œ/ì—£ì§€ëŠ” ì†ì‹¤ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡ ì„ì˜ ê°’(=uniform)ìœ¼ë¡œ ì„¤ì •.
        probX0[~node_mask] = torch.ones(self.Xdim_output).type_as(probX0)
        probE0[~(node_mask.unsqueeze(1) * node_mask.unsqueeze(2))] = torch.ones(self.Edim_output).type_as(probE0)

        # ìê¸° ìì‹ ìœ¼ë¡œì˜ ì—£ì§€(self-loop)ëŠ” ë¬´ì‹œ
        diag_mask = torch.eye(probE0.size(1)).type_as(probE0).bool()
        diag_mask = diag_mask.unsqueeze(0).expand(probE0.size(0), -1, -1)
        probE0[diag_mask] = torch.ones(self.Edim_output).type_as(probE0)

        # softmax í™•ë¥ ë“¤ì„ ë°˜í™˜
        return utils.PlaceHolder(X=probX0, E=probE0, y=proby0)

    ##### apply_noise ì„¤ëª… #####
    # ê° ë¶„ì xì— ëŒ€í•´ ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ì˜ timestep të¥¼ ì„ íƒí•˜ì—¬ í•´ë‹¹ ì‹œì  zt ì˜ noisy ë°ì´í„° ë¥¼ ë§Œë“¤ê³ , ì´ë¡œë¶€í„° clean data xë¥¼ ë³µì›í•˜ëŠ” ê²ƒì„ í•™ìŠµí•©ë‹ˆë‹¤.
    # ë¶„ì (ìƒ˜í”Œ) inde, ìƒ˜í”Œë§ëœ t
    # 0	37
    # 1	92
    # 2	14
    # 3	51
    #
    # ëª¨ë“  tì— ëŒ€í•´ í•œ ë²ˆì— í•™ìŠµí•˜ëŠ” ê±´ ë¹„íš¨ìœ¨ì 
    # ëŒ€ì‹  ë§¤ epochë§ˆë‹¤ ë‹¤ë¥¸ të¥¼ ëœë¤í•˜ê²Œ ì„ íƒ â†’ ì „ì²´ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹œì ì„ í•™ìŠµí•˜ê²Œ ë¨
    # ì´ê²ƒì´ "random timestep training", DDPMì˜ í•µì‹¬ ì „ëµ ì¤‘ í•˜ë‚˜
    ####################################

    def apply_noise(self, X, E, y, node_mask): # ìœ„ìª½ì— train, val, test ìª½ì—ì„œ ì‚¬ìš©ë¨
        """ Sample noise and apply it to the data. """

        ### Sample a timestep t. | diffusion ì‹œì  t ìƒ˜í”Œë§
        ### When evaluating, the loss for t=0 is computed separately | í•™ìŠµ ì¤‘ì—ëŠ” t=0âˆ¼T, í…ŒìŠ¤íŠ¸/validation ì‹œì—ëŠ” t=1âˆ¼Të§Œ (t=0ì€ í‰ê°€ìš©)
        lowest_t = 0 if self.training else 1
        # lowest_t ~ t+1 ì‚¬ì´ì˜ ì •ìˆ˜ì¤‘ ëœë¤/ ê° ìƒ˜í”Œì— ëŒ€í•´ 1ê°œì˜ tê°’ì„ í• ë‹¹ : size=(X.size(0), 1) : batch size,1
        t_int = torch.randint(lowest_t, self.T + 1, size=(X.size(0), 1), device=X.device).float()  # (bs, 1) | lowest_t ì—ì„œ self.T + 1 ê¹Œì§€
        s_int = t_int - 1 # í˜„ì¬ ì‹œì ë³´ë‹¤ ì´ì „ì‹œì 

        t_float = t_int / self.T
        s_float = s_int / self.T

        ### beta_t and alpha_s_bar are used for denoising/loss computation | ì•ŒíŒŒ/ë² íƒ€ ê°’ ê³„ì‚° (ìŠ¤ì¼€ì¤„ë§)
        # self.noise_schedule = src.diffusion.noise_schedule PredefinedNoiseScheduleDiscrete
        beta_t = self.noise_schedule(t_normalized=t_float)                         # (bs, 1)
        alpha_s_bar = self.noise_schedule.get_alpha_bar(t_normalized=s_float)      # (bs, 1)
        alpha_t_bar = self.noise_schedule.get_alpha_bar(t_normalized=t_float)      # (bs, 1)

        ## ì „ì´ í™•ë¥  í–‰ë ¬ Qt ìƒì„±
        # from src.diffusion.noise_schedule DiscreteUniformTransition ë˜ëŠ” MarginalUniformTransition
        Qtb = self.transition_model.get_Qt_bar(alpha_t_bar, device=self.device)  # (bs, dx_in, dx_out), (bs, de_in, de_out)
        assert (abs(Qtb.X.sum(dim=2) - 1.) < 1e-4).all(), Qtb.X.sum(dim=2) - 1
        assert (abs(Qtb.E.sum(dim=2) - 1.) < 1e-4).all()

        ### Compute transition probabilities | ë…¸ì´ì¦ˆ ì¶”ê°€
        # @ : í–‰ë ¬ ê³± ì—°ì‚°ì, torch.matmul() ë˜ëŠ” np.matmul()ì„ í˜¸ì¶œ
        probX = X @ Qtb.X  # (bs, n, dx_out)
        probE = E @ Qtb.E.unsqueeze(1)  # (bs, n, n, de_out)

        sampled_t = diffusion_utils.sample_discrete_features(probX=probX, probE=probE, node_mask=node_mask)

        # ì›-í•« ì¸ì½”ë”©
        X_t = F.one_hot(sampled_t.X, num_classes=self.Xdim_output)
        E_t = F.one_hot(sampled_t.E, num_classes=self.Edim_output)
        assert (X.shape == X_t.shape) and (E.shape == E_t.shape)

        z_t = utils.PlaceHolder(X=X_t, E=E_t, y=y).type_as(X_t).mask(node_mask)

        noisy_data = {'t_int': t_int, 't': t_float, 'beta_t': beta_t, 'alpha_s_bar': alpha_s_bar,
                      'alpha_t_bar': alpha_t_bar, 'X_t': z_t.X, 'E_t': z_t.E, 'y_t': z_t.y, 'node_mask': node_mask}
        return noisy_data

    def compute_val_loss(self, pred, noisy_data, X, E, y, node_mask, test=False): # ìœ„ìª½ val, test ì—ì„œ ì‚¬ìš©
        """Computes an estimator for the variational lower bound, or the simple loss (MSE).
           pred: (batch_size, n, total_features)
           noisy_data: dict
           X, E, y : (bs, n, dx),  (bs, n, n, de), (bs, dy)
           node_mask : (bs, n)
           Output: nll (size 1)
       """
        t = noisy_data['t']

        ### 1. ###
        # ì‚¬ì „ í™•ë¥ ë¶„í¬ ğ‘(ğ‘) : node count ë¶„í¬, ì¦‰ ê·¸ë˜í”„, ë¶„ìì˜ ë…¸ë“œ ê°œìˆ˜ëŠ” ëª‡ê°œ ì¼ê²ƒì¸ê°€? ì— ëŒ€í•œ ë¶„í¬
        # ìƒ˜í”Œë³„ ë…¸ë“œ ìˆ˜ì˜ ìš°ë„ (likehood)
        N = node_mask.sum(1).long() # ê° ìƒ˜í”Œì˜ ìœ íš¨ ë…¸ë“œ ìˆ˜
        log_pN = self.node_dist.log_prob(N) # ì‚¬ì „ ë¶„í¬ë¡œë¶€í„° í™•ë¥ (log), ë…¸ë“œ ê°œìˆ˜ì— ëŒ€í•œ ì´ì‚° í™•ë¥  ë¶„í¬(discrete distribution)

        ### 2. The KL between q(z_T | x) and p(z_T) = Uniform(1/num_classes). Should be close to zero. ###
        # KL Divergence (prior vs posterior)
        kl_prior = self.kl_prior(X, E, y, node_mask) # ì´ˆê¸° ë…¸ì´ì¦ˆ ë¶„í¬ê°€ priorì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ì¸¡ì •

        ### 3. Diffusion loss ###
        # Diffusion ë‹¨ê³„ ì†ì‹¤ (ì „ì²´ ì‹œì  í†µí•©)
        # ëª¨ë¸ì´ ê° ì‹œì  tì—ì„œ z_t â†’ z_{t-1} ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€
        # ê° ìƒ˜í”Œë³„ë¡œ ì—¬ëŸ¬ tì—ì„œ í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„
        # ë³´í†µ CrossEntropy ë˜ëŠ” MSEë¡œ ê³„ì‚°
        loss_all_t = self.compute_Lt(X, E, y, pred, noisy_data, node_mask, test)

        ### 4. Reconstruction loss ###
        # Compute L0 term : -log p (X, E, y | z_0) = reconstruction loss
        # ëª¨ë¸ì´ t=0 ì‹œì ì—ì„œ ì›ë˜ì˜ ë°ì´í„° (X, E, y) ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë³µì›í•˜ëŠ”ì§€
        prob0 = self.reconstruction_logp(t, X, E, y, node_mask)

        loss_term_0 = self.val_X_logp(X * prob0.X.log()) + self.val_E_logp(E * prob0.E.log()) + \
                      self.val_y_logp(y * prob0.y.log())

        ### Combine terms | NLL í†µí•© ê³„ì‚° ###
        nlls = - log_pN + kl_prior + loss_all_t - loss_term_0
        assert len(nlls.shape) == 1, f'{nlls.shape} has more than only batch dim.'

        ### Update NLL metric object and return batch nll ###
        # from src.metrics.abstract_metrics import NLL
        nll = (self.test_nll if test else self.val_nll)(nlls)        # Average over the batch

        # wandb ë¡œê¹…
        #wandb.log({"kl prior": kl_prior.mean(),
        #           "Estimator loss terms": loss_all_t.mean(),
        #           "log_pn": log_pN.mean(),
        #           "loss_term_0": loss_term_0,
        #           'test_nll' if test else 'val_nll': nll}, commit=False)

        log_data = {
            "epoch": self.current_epoch,
            "kl_prior": kl_prior.mean().item(),
            "loss_all_t": loss_all_t.mean().item(),
            "log_pn": log_pN.mean().item(),
            "loss_term_0": loss_term_0.item() if isinstance(loss_term_0, torch.Tensor) else loss_term_0,
            "nll": nll.item()
        }
        log_to_csv(log_data, file_path="logs/val_log.csv")


        return nll

    # ëª¨ë¸ ì˜ˆì¸¡(denoising) ì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ë¶€ë¶„
    def forward(self, noisy_data, extra_data, node_mask):
        X = torch.cat((noisy_data['X_t'], extra_data.X), dim=2).float()
        E = torch.cat((noisy_data['E_t'], extra_data.E), dim=3).float()
        y = torch.hstack((noisy_data['y_t'], extra_data.y)).float()
        return self.model(X, E, y, node_mask) # self.model = GraphTransformer


    # ì´ sample_batch í•¨ìˆ˜ëŠ” Diffusion ëª¨ë¸ì—ì„œ moleculeì„ ìƒ˜í”Œë§(ìƒì„±)í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹´ë‹¹
    # Diffusion ëª¨ë¸ì˜ í•™ìŠµì´ ëë‚œ ë’¤,
    # ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ë…¸ì´ì¦ˆ ìƒíƒœ z_Të¡œë¶€í„° zâ‚€ê¹Œì§€ ì—­ì¶”ë¡ (reverse diffusion) ê³¼ì •ì„ í†µí•´ ìƒˆë¡œìš´ ë¶„ì êµ¬ì¡°ë¥¼ ìƒì„±
    @torch.no_grad()
    def sample_batch(self, batch_id: int, batch_size: int, keep_chain: int, number_chain_steps: int,
                     save_final: int, num_nodes=None): # test_epoch_end, val_epoch_end
        """
        :param batch_id: int
        :param batch_size: int
        :param num_nodes: int, <int>tensor (batch_size) (optional) for specifying number of nodes
        :param save_final: int: number of predictions to save to file
        :param keep_chain: int: number of chains to save to file
        :param keep_chain_steps: number of timesteps to save for each chain
        :return: molecule_list. Each element of this list is a tuple (atom_types, charges, positions)
        """
        if num_nodes is None:
            n_nodes = self.node_dist.sample_n(batch_size, self.device)
        elif type(num_nodes) == int:
            n_nodes = num_nodes * torch.ones(batch_size, device=self.device, dtype=torch.int)
        else:
            assert isinstance(num_nodes, torch.Tensor)
            n_nodes = num_nodes
        n_max = torch.max(n_nodes).item()
        # Build the masks
        arange = torch.arange(n_max, device=self.device).unsqueeze(0).expand(batch_size, -1)
        node_mask = arange < n_nodes.unsqueeze(1)
        # TODO: how to move node_mask on the right device in the multi-gpu case?
        # TODO: everything else depends on its device
        # Sample noise  -- z has size (n_samples, n_nodes, n_features)
        z_T = diffusion_utils.sample_discrete_feature_noise(limit_dist=self.limit_dist, node_mask=node_mask)
        X, E, y = z_T.X, z_T.E, z_T.y

        assert (E == torch.transpose(E, 1, 2)).all()
        assert number_chain_steps < self.T
        chain_X_size = torch.Size((number_chain_steps, keep_chain, X.size(1)))
        chain_E_size = torch.Size((number_chain_steps, keep_chain, E.size(1), E.size(2)))

        chain_X = torch.zeros(chain_X_size)
        chain_E = torch.zeros(chain_E_size)

        # Iteratively sample p(z_s | z_t) for t = 1, ..., T, with s = t - 1.
        for s_int in reversed(range(0, self.T)):
            s_array = s_int * torch.ones((batch_size, 1)).type_as(y)
            t_array = s_array + 1
            s_norm = s_array / self.T
            t_norm = t_array / self.T

            # Sample z_s
            sampled_s, discrete_sampled_s, predicted_graph = self.sample_p_zs_given_zt(t_norm, X, E, y, node_mask,
                                                                                       last_step=s_int == 100)
            X, E, y = sampled_s.X, sampled_s.E, sampled_s.y

            # Save the first keep_chain graphs
            write_index = (s_int * number_chain_steps) // self.T
            chain_X[write_index] = discrete_sampled_s.X[:keep_chain]
            chain_E[write_index] = discrete_sampled_s.E[:keep_chain]

        # Sample
        sampled_s = sampled_s.mask(node_mask, collapse=True)
        X, E, y = sampled_s.X, sampled_s.E, sampled_s.y



        # Prepare the chain for saving
        if keep_chain > 0:
            final_X_chain = X[:keep_chain]
            final_E_chain = E[:keep_chain]

            chain_X[0] = final_X_chain                  # Overwrite last frame with the resulting X, E
            chain_E[0] = final_E_chain

            chain_X = diffusion_utils.reverse_tensor(chain_X)
            chain_E = diffusion_utils.reverse_tensor(chain_E)

            # Repeat last frame to see final sample better
            chain_X = torch.cat([chain_X, chain_X[-1:].repeat(10, 1, 1)], dim=0)
            chain_E = torch.cat([chain_E, chain_E[-1:].repeat(10, 1, 1, 1)], dim=0)
            assert chain_X.size(0) == (number_chain_steps + 10)

        molecule_list = []
        for i in range(batch_size):
            n = n_nodes[i]
            atom_types = X[i, :n].cpu()
            edge_types = E[i, :n, :n].cpu()
            molecule_list.append([atom_types, edge_types])
            if i < 3:
                print("Example of generated E: ", atom_types)
                print("Example of generated X: ", edge_types)

        predicted_graph_list = []
        for i in range(batch_size):
            n = n_nodes[i]
            atom_types = X[i, :n].cpu()
            edge_types = E[i, :n, :n].cpu()
            predicted_graph_list.append([atom_types, edge_types])


        # Visualize chains
        if self.visualization_tools is not None:
            print('Visualizing chains...')
            current_path = os.getcwd()
            num_molecules = chain_X.size(1)       # number of molecules
            for i in range(num_molecules):
                result_path = os.path.join(current_path, f'chains/{self.cfg.general.name}/'
                                                         f'epoch{self.current_epoch}/'
                                                         f'chains/molecule_{batch_id + i}')
                if not os.path.exists(result_path):
                    os.makedirs(result_path)
                    _ = self.visualization_tools.visualize_chain(result_path,
                                                                 chain_X[:, i, :].numpy(),
                                                                 chain_E[:, i, :].numpy())
                print('\r{}/{} complete'.format(i+1, num_molecules), end='', flush=True)
            print('\nVisualizing molecules...')

            # Visualize the final molecules
            current_path = os.getcwd()
            result_path = os.path.join(current_path,
                                       f'graphs/{self.name}/epoch{self.current_epoch}_b{batch_id}/')
            self.visualization_tools.visualize(result_path, molecule_list, save_final)
            self.visualization_tools.visualize(result_path, predicted_graph_list, save_final, log='predicted')
            print("Done.")

        return molecule_list

    # ì´ í•¨ìˆ˜ sample_p_zs_given_zt()ëŠ” reverse diffusion ê³¼ì •ì˜ í•µì‹¬ìœ¼ë¡œ,
    # ì£¼ì–´ì§„ noisy ìƒíƒœ ğ‘§ğ‘¡ì—ì„œ í•œ ë‹¨ê³„ ì „ ìƒíƒœğ‘§ğ‘  (s=tâˆ’1)ë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
    def sample_p_zs_given_zt(self, t, X_t, E_t, y_t, node_mask, last_step: bool): # sample_batch ì—ì„œ ì‚¬ìš©
        """Samples from zs ~ p(zs | zt). Only used during sampling.
           if last_step, return the graph prediction as well"""
        bs, n, dxs = X_t.shape
        beta_t = self.noise_schedule(t_normalized=t)  # (bs, 1)
        alpha_s_bar = self.noise_schedule.get_alpha_bar(t_normalized=t)
        alpha_t_bar = self.noise_schedule.get_alpha_bar(t_normalized=t)

        # Retrieve transitions matrix
        Qtb = self.transition_model.get_Qt_bar(alpha_t_bar, self.device)
        Qsb = self.transition_model.get_Qt_bar(alpha_s_bar, self.device)
        Qt = self.transition_model.get_Qt(beta_t, self.device)

        # Neural net predictions
        noisy_data = {'X_t': X_t, 'E_t': E_t, 'y_t': y_t, 't': t, 'node_mask': node_mask}
        extra_data = self.compute_extra_data(noisy_data)
        pred = self.forward(noisy_data, extra_data, node_mask)

        # Normalize predictions
        pred_X = F.softmax(pred.X, dim=-1)               # bs, n, d0
        pred_E = F.softmax(pred.E, dim=-1)               # bs, n, n, d0

        if last_step:
            predicted_graph = diffusion_utils.sample_discrete_features(pred_X, pred_E, node_mask=node_mask)

        p_s_and_t_given_0_X = diffusion_utils.compute_batched_over0_posterior_distribution(X_t=X_t,
                                                                                           Qt=Qt.X,
                                                                                           Qsb=Qsb.X,
                                                                                           Qtb=Qtb.X)

        p_s_and_t_given_0_E = diffusion_utils.compute_batched_over0_posterior_distribution(X_t=E_t,
                                                                                           Qt=Qt.E,
                                                                                           Qsb=Qsb.E,
                                                                                           Qtb=Qtb.E)
        # Dim of these two tensors: bs, N, d0, d_t-1
        weighted_X = pred_X.unsqueeze(-1) * p_s_and_t_given_0_X         # bs, n, d0, d_t-1
        unnormalized_prob_X = weighted_X.sum(dim=2)                     # bs, n, d_t-1
        unnormalized_prob_X[torch.sum(unnormalized_prob_X, dim=-1) == 0] = 1e-5
        prob_X = unnormalized_prob_X / torch.sum(unnormalized_prob_X, dim=-1, keepdim=True)  # bs, n, d_t-1

        pred_E = pred_E.reshape((bs, -1, pred_E.shape[-1]))
        weighted_E = pred_E.unsqueeze(-1) * p_s_and_t_given_0_E        # bs, N, d0, d_t-1
        unnormalized_prob_E = weighted_E.sum(dim=-2)
        unnormalized_prob_E[torch.sum(unnormalized_prob_E, dim=-1) == 0] = 1e-5
        prob_E = unnormalized_prob_E / torch.sum(unnormalized_prob_E, dim=-1, keepdim=True)
        prob_E = prob_E.reshape(bs, n, n, pred_E.shape[-1])

        assert ((prob_X.sum(dim=-1) - 1).abs() < 1e-4).all()
        assert ((prob_E.sum(dim=-1) - 1).abs() < 1e-4).all()

        sampled_s = diffusion_utils.sample_discrete_features(prob_X, prob_E, node_mask=node_mask)

        X_s = F.one_hot(sampled_s.X, num_classes=self.Xdim_output).float()
        E_s = F.one_hot(sampled_s.E, num_classes=self.Edim_output).float()

        assert (E_s == torch.transpose(E_s, 1, 2)).all()
        assert (X_t.shape == X_s.shape) and (E_t.shape == E_s.shape)

        out_one_hot = utils.PlaceHolder(X=X_s, E=E_s, y=torch.zeros(y_t.shape[0], 0))
        out_discrete = utils.PlaceHolder(X=X_s, E=E_s, y=torch.zeros(y_t.shape[0], 0))

        return out_one_hot.mask(node_mask).type_as(y_t), out_discrete.mask(node_mask, collapse=True).type_as(y_t), \
               predicted_graph if last_step else None

    def compute_extra_data(self, noisy_data): #training_setp, Validation_step, test_step, reconstruction_logp, sample_p_zs_given_zt
        """ At every training step (after adding noise) and step in sampling, compute extra information and append to
            the network input. """
        # src -> diffusion -> extra features -> DummyExtraFeatures Class, ExtraFeatures Class ë‚´ì—ì„œ ExtraFeatures typeì— ë”°ë¼ ë‹¤ë¥´ê²Œ ê³„ì‚°
        extra_features = self.extra_features(noisy_data)
        # src -> diffusion -> extra features molecular -> ExtraMolecularFeatures Class ë‚´ì—ì„œ ExtraFeatures typeì— ë”°ë¼ ë‹¤ë¥´ê²Œ ê³„ì‚°
        extra_molecular_features = self.domain_features(noisy_data)

        # ìœ„ì—ì„œ ì •ì˜ëœ Class í™œìš©í•´ features ê³„ì‚°
        extra_X = torch.cat((extra_features.X, extra_molecular_features.X), dim=-1)
        extra_E = torch.cat((extra_features.E, extra_molecular_features.E), dim=-1)
        extra_y = torch.cat((extra_features.y, extra_molecular_features.y), dim=-1)

        t = noisy_data['t'] # í˜„ì¬ diffusion timestep
        extra_y = torch.cat((extra_y, t), dim=1) # ì „ì—­ íŠ¹ì„±ì— t ê°’ì„ ì¶”ê°€ë¡œ ë¶™ì„

        return utils.PlaceHolder(X=extra_X, E=extra_E, y=extra_y)
